{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming you have a DataFrame named 'd' or modify this line to load your data\n",
    "df = pd.read_csv('Assets/prefinal_dataset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>SESSION_ID</th>\n",
       "      <th>TRIAL_ID</th>\n",
       "      <th>COP_AP_PRO_1</th>\n",
       "      <th>COP_AP_PRO_2</th>\n",
       "      <th>COP_AP_PRO_3</th>\n",
       "      <th>COP_AP_PRO_4</th>\n",
       "      <th>COP_AP_PRO_5</th>\n",
       "      <th>COP_AP_PRO_6</th>\n",
       "      <th>COP_AP_PRO_7</th>\n",
       "      <th>...</th>\n",
       "      <th>F_V_PRO_93.1</th>\n",
       "      <th>F_V_PRO_94.1</th>\n",
       "      <th>F_V_PRO_95.1</th>\n",
       "      <th>F_V_PRO_96.1</th>\n",
       "      <th>F_V_PRO_97.1</th>\n",
       "      <th>F_V_PRO_98.1</th>\n",
       "      <th>F_V_PRO_99.1</th>\n",
       "      <th>F_V_PRO_100.1</th>\n",
       "      <th>F_V_PRO_101.1</th>\n",
       "      <th>CLASS_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510</td>\n",
       "      <td>413</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.009565</td>\n",
       "      <td>0.014452</td>\n",
       "      <td>0.019158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282119</td>\n",
       "      <td>0.233682</td>\n",
       "      <td>0.189883</td>\n",
       "      <td>0.151416</td>\n",
       "      <td>0.117502</td>\n",
       "      <td>0.087352</td>\n",
       "      <td>0.061159</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510</td>\n",
       "      <td>413</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0.017909</td>\n",
       "      <td>0.023363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271192</td>\n",
       "      <td>0.225628</td>\n",
       "      <td>0.184277</td>\n",
       "      <td>0.147850</td>\n",
       "      <td>0.115619</td>\n",
       "      <td>0.086782</td>\n",
       "      <td>0.061318</td>\n",
       "      <td>0.039709</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510</td>\n",
       "      <td>413</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.004852</td>\n",
       "      <td>0.009656</td>\n",
       "      <td>0.014573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285054</td>\n",
       "      <td>0.235092</td>\n",
       "      <td>0.189484</td>\n",
       "      <td>0.149884</td>\n",
       "      <td>0.115968</td>\n",
       "      <td>0.086485</td>\n",
       "      <td>0.060872</td>\n",
       "      <td>0.039427</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510</td>\n",
       "      <td>413</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>0.013107</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.023304</td>\n",
       "      <td>0.027571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251956</td>\n",
       "      <td>0.202682</td>\n",
       "      <td>0.159656</td>\n",
       "      <td>0.124149</td>\n",
       "      <td>0.095263</td>\n",
       "      <td>0.071202</td>\n",
       "      <td>0.051078</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510</td>\n",
       "      <td>413</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.018278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256634</td>\n",
       "      <td>0.212920</td>\n",
       "      <td>0.174113</td>\n",
       "      <td>0.139937</td>\n",
       "      <td>0.109247</td>\n",
       "      <td>0.081570</td>\n",
       "      <td>0.057560</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.022633</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1014 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SUBJECT_ID  SESSION_ID  TRIAL_ID  COP_AP_PRO_1  COP_AP_PRO_2  COP_AP_PRO_3  \\\n",
       "0         510         413         1             0           0.0      0.001679   \n",
       "1         510         413         2             0          -0.0      0.001762   \n",
       "2         510         413         3             0           0.0     -0.000709   \n",
       "3         510         413         4             0          -0.0      0.006717   \n",
       "4         510         413         6             0          -0.0      0.000983   \n",
       "\n",
       "   COP_AP_PRO_4  COP_AP_PRO_5  COP_AP_PRO_6  COP_AP_PRO_7  ...  F_V_PRO_93.1  \\\n",
       "0      0.005040      0.009565      0.014452      0.019158  ...      0.282119   \n",
       "1      0.006239      0.012014      0.017909      0.023363  ...      0.271192   \n",
       "2      0.000988      0.004852      0.009656      0.014573  ...      0.285054   \n",
       "3      0.013107      0.018519      0.023304      0.027571  ...      0.251956   \n",
       "4      0.004263      0.008647      0.013425      0.018278  ...      0.256634   \n",
       "\n",
       "   F_V_PRO_94.1  F_V_PRO_95.1  F_V_PRO_96.1  F_V_PRO_97.1  F_V_PRO_98.1  \\\n",
       "0      0.233682      0.189883      0.151416      0.117502      0.087352   \n",
       "1      0.225628      0.184277      0.147850      0.115619      0.086782   \n",
       "2      0.235092      0.189484      0.149884      0.115968      0.086485   \n",
       "3      0.202682      0.159656      0.124149      0.095263      0.071202   \n",
       "4      0.212920      0.174113      0.139937      0.109247      0.081570   \n",
       "\n",
       "   F_V_PRO_99.1  F_V_PRO_100.1  F_V_PRO_101.1  CLASS_LABEL  \n",
       "0      0.061159       0.039500       0.022633            C  \n",
       "1      0.061318       0.039709       0.022630            C  \n",
       "2      0.060872       0.039427       0.022631            C  \n",
       "3      0.051078       0.034963       0.022631            C  \n",
       "4      0.057560       0.037933       0.022633            C  \n",
       "\n",
       "[5 rows x 1014 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_map = {'HC': 0, 'A': 1, 'H': 1, 'C': 1, 'K': 1}\n",
    "\n",
    "# Map the values in the CLASS_LABEL column using label_map\n",
    "df['CLASS_LABEL'] = df['CLASS_LABEL'].map(label_map)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each class label\n",
    "class_counts = df['CLASS_LABEL'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Class Label Distribution')\n",
    "\n",
    "# Display the pie chart\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Assuming you have a DataFrame named 'd' or modify this line to load your data\n",
    "df = pd.read_csv('Assets/final_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135954, 23)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "X = df[['COP_AP_PRO_2', 'COP_AP_PRO_3', 'COP_AP_PRO_2.1', 'COP_AP_PRO_3.1', 'COP_ML_PRO_1', 'COP_ML_PRO_1.1', \n",
    "'F_AP_PRO_1', 'F_AP_PRO_2', 'F_AP_PRO_3', 'F_AP_PRO_1.1', 'F_AP_PRO_2.1', 'F_AP_PRO_3.1', 'F_ML_PRO_1',\n",
    "'F_ML_PRO_2', 'F_ML_PRO_8', 'F_ML_PRO_9', 'F_ML_PRO_1.1', 'F_ML_PRO_2.1', 'F_ML_PRO_8.1', 'F_ML_PRO_9.1', \n",
    "'F_V_PRO_1', 'F_V_PRO_2', 'F_V_PRO_2.1']]\n",
    "y = df['CLASS_LABEL']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each class label\n",
    "class_counts = df['CLASS_LABEL'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Class Label Distribution')\n",
    "\n",
    "# Display the pie chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply Standard Scaling to the feature variables\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 100, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "Accuracy on test set: 78.65%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear'], 'gamma': ['auto']}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py\", line 180, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'gamma' parameter of SVC must be a str among {'auto', 'scale'} or a float in the range [0.0, inf). Got 'scaler' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.78323371        nan 0.78390621        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 100, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "Accuracy on test set: 78.65%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the SVM classifier\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [1000,100], 'kernel': ['linear'], 'gamma': ['auto','scaler']}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "35 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.76906915        nan 0.77300959        nan 0.77282045\n",
      "        nan 0.77318823        nan 0.77899906        nan 0.78109011\n",
      "        nan 0.78135282]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 1000, 'penalty': 'l2'}\n",
      "Accuracy on test set: 78.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chand\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define the Logistic Regression classifier\n",
    "logreg_classifier = LogisticRegression()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(logreg_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chand\\OneDrive\\Desktop\\Gait Project\\Code\\modeltesting.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(rf_classifier, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Fit the model to the training data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Get the best parameters from the grid search\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m best_params \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    890\u001b[0m         X,\n\u001b[0;32m    891\u001b[0m         y,\n\u001b[0;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 97.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "# Define the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100,max_depth=20, random_state=42)\n",
    "\n",
    "# Assuming you have already defined and split your features and target variables (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100 estimator = 98.37%\n",
    "200 estimator = 98.44%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli -> 67.53%\n",
    "# Gaussian -> 76.18%\n",
    "# Multinomial -> doesnot work on negative value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "92.20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.6876 - accuracy: 0.6161 - val_loss: 0.6825 - val_accuracy: 0.7061\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.6697 - accuracy: 0.7449 - val_loss: 0.6497 - val_accuracy: 0.7616\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.5881 - accuracy: 0.7674 - val_loss: 0.5620 - val_accuracy: 0.6917\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.4968 - accuracy: 0.7685 - val_loss: 0.5931 - val_accuracy: 0.7130\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.4865 - accuracy: 0.7717 - val_loss: 0.5212 - val_accuracy: 0.7483\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4839 - accuracy: 0.7732 - val_loss: 0.4967 - val_accuracy: 0.7642\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4823 - accuracy: 0.7745 - val_loss: 0.5102 - val_accuracy: 0.7614\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4808 - accuracy: 0.7750 - val_loss: 0.5102 - val_accuracy: 0.7591\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4799 - accuracy: 0.7752 - val_loss: 0.5247 - val_accuracy: 0.7493\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4788 - accuracy: 0.7759 - val_loss: 0.4773 - val_accuracy: 0.7744\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4780 - accuracy: 0.7757 - val_loss: 0.5257 - val_accuracy: 0.7398\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.4771 - accuracy: 0.7762 - val_loss: 0.4804 - val_accuracy: 0.7744\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4762 - accuracy: 0.7764 - val_loss: 0.4777 - val_accuracy: 0.7739\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4755 - accuracy: 0.7769 - val_loss: 0.4857 - val_accuracy: 0.7718\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4745 - accuracy: 0.7777 - val_loss: 0.4920 - val_accuracy: 0.7720\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4736 - accuracy: 0.7777 - val_loss: 0.6289 - val_accuracy: 0.6362\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4734 - accuracy: 0.7787 - val_loss: 0.4942 - val_accuracy: 0.7666\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4723 - accuracy: 0.7784 - val_loss: 0.5622 - val_accuracy: 0.7138\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4709 - accuracy: 0.7786 - val_loss: 0.4865 - val_accuracy: 0.7730\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4711 - accuracy: 0.7797 - val_loss: 0.4797 - val_accuracy: 0.7757\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,  # Split a portion of training data for validation\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4683 - accuracy: 0.7819 - val_loss: 0.4548 - val_accuracy: 0.7882\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4431 - accuracy: 0.7949 - val_loss: 0.4347 - val_accuracy: 0.8009\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4327 - accuracy: 0.7997 - val_loss: 0.4494 - val_accuracy: 0.7893\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.4255 - accuracy: 0.8041 - val_loss: 0.4405 - val_accuracy: 0.7985\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4192 - accuracy: 0.8064 - val_loss: 0.4497 - val_accuracy: 0.7869\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.4140 - accuracy: 0.8093 - val_loss: 0.4200 - val_accuracy: 0.8090\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.4096 - accuracy: 0.8100 - val_loss: 0.4057 - val_accuracy: 0.8119\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4065 - accuracy: 0.8111 - val_loss: 0.4066 - val_accuracy: 0.8116\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4022 - accuracy: 0.8136 - val_loss: 0.3984 - val_accuracy: 0.8144\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3981 - accuracy: 0.8143 - val_loss: 0.4085 - val_accuracy: 0.8113\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3939 - accuracy: 0.8172 - val_loss: 0.4042 - val_accuracy: 0.8145\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3914 - accuracy: 0.8183 - val_loss: 0.4134 - val_accuracy: 0.8099\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3875 - accuracy: 0.8191 - val_loss: 0.3867 - val_accuracy: 0.8197\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3853 - accuracy: 0.8203 - val_loss: 0.3921 - val_accuracy: 0.8192\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3826 - accuracy: 0.8215 - val_loss: 0.3828 - val_accuracy: 0.8222\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3789 - accuracy: 0.8221 - val_loss: 0.3825 - val_accuracy: 0.8242\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3751 - accuracy: 0.8249 - val_loss: 0.3766 - val_accuracy: 0.8244\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3723 - accuracy: 0.8259 - val_loss: 0.3820 - val_accuracy: 0.8230\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3685 - accuracy: 0.8297 - val_loss: 0.3757 - val_accuracy: 0.8263\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3660 - accuracy: 0.8291 - val_loss: 0.3701 - val_accuracy: 0.8254\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3622 - accuracy: 0.8316 - val_loss: 0.3815 - val_accuracy: 0.8249\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3589 - accuracy: 0.8337 - val_loss: 0.3847 - val_accuracy: 0.8236\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3561 - accuracy: 0.8342 - val_loss: 0.3674 - val_accuracy: 0.8274\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3523 - accuracy: 0.8374 - val_loss: 0.3711 - val_accuracy: 0.8285\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3492 - accuracy: 0.8373 - val_loss: 0.3643 - val_accuracy: 0.8302\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3472 - accuracy: 0.8403 - val_loss: 0.3677 - val_accuracy: 0.8278\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3431 - accuracy: 0.8411 - val_loss: 0.3582 - val_accuracy: 0.8332\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3404 - accuracy: 0.8423 - val_loss: 0.3665 - val_accuracy: 0.8293\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3376 - accuracy: 0.8430 - val_loss: 0.3486 - val_accuracy: 0.8394\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3334 - accuracy: 0.8474 - val_loss: 0.3552 - val_accuracy: 0.8355\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3307 - accuracy: 0.8477 - val_loss: 0.3476 - val_accuracy: 0.8418\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3264 - accuracy: 0.8496 - val_loss: 0.3453 - val_accuracy: 0.8410\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3251 - accuracy: 0.8512 - val_loss: 0.3434 - val_accuracy: 0.8428\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3205 - accuracy: 0.8529 - val_loss: 0.3409 - val_accuracy: 0.8444\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3171 - accuracy: 0.8547 - val_loss: 0.3417 - val_accuracy: 0.8452\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3148 - accuracy: 0.8570 - val_loss: 0.3391 - val_accuracy: 0.8433\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3121 - accuracy: 0.8576 - val_loss: 0.3441 - val_accuracy: 0.8407\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3088 - accuracy: 0.8588 - val_loss: 0.3342 - val_accuracy: 0.8491\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3054 - accuracy: 0.8603 - val_loss: 0.3267 - val_accuracy: 0.8524\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3028 - accuracy: 0.8631 - val_loss: 0.3283 - val_accuracy: 0.8511\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2995 - accuracy: 0.8630 - val_loss: 0.3298 - val_accuracy: 0.8495\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2977 - accuracy: 0.8651 - val_loss: 0.3302 - val_accuracy: 0.8517\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2935 - accuracy: 0.8665 - val_loss: 0.3272 - val_accuracy: 0.8533\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2924 - accuracy: 0.8680 - val_loss: 0.3300 - val_accuracy: 0.8474\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2890 - accuracy: 0.8688 - val_loss: 0.3481 - val_accuracy: 0.8475\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2859 - accuracy: 0.8705 - val_loss: 0.3207 - val_accuracy: 0.8581\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2829 - accuracy: 0.8724 - val_loss: 0.3226 - val_accuracy: 0.8554\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2798 - accuracy: 0.8744 - val_loss: 0.3168 - val_accuracy: 0.8566\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2774 - accuracy: 0.8751 - val_loss: 0.3176 - val_accuracy: 0.8572\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2756 - accuracy: 0.8752 - val_loss: 0.3243 - val_accuracy: 0.8564\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2727 - accuracy: 0.8772 - val_loss: 0.3142 - val_accuracy: 0.8585\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2697 - accuracy: 0.8797 - val_loss: 0.3376 - val_accuracy: 0.8478\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2691 - accuracy: 0.8791 - val_loss: 0.3082 - val_accuracy: 0.8637\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2664 - accuracy: 0.8798 - val_loss: 0.3241 - val_accuracy: 0.8569\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2630 - accuracy: 0.8829 - val_loss: 0.3125 - val_accuracy: 0.8627\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2608 - accuracy: 0.8826 - val_loss: 0.3147 - val_accuracy: 0.8606\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2576 - accuracy: 0.8851 - val_loss: 0.3170 - val_accuracy: 0.8590\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2558 - accuracy: 0.8855 - val_loss: 0.3051 - val_accuracy: 0.8672\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2529 - accuracy: 0.8873 - val_loss: 0.3037 - val_accuracy: 0.8650\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2505 - accuracy: 0.8884 - val_loss: 0.3154 - val_accuracy: 0.8601\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2484 - accuracy: 0.8902 - val_loss: 0.3086 - val_accuracy: 0.8681\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2458 - accuracy: 0.8909 - val_loss: 0.3102 - val_accuracy: 0.8649\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2442 - accuracy: 0.8907 - val_loss: 0.2982 - val_accuracy: 0.8702\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2408 - accuracy: 0.8944 - val_loss: 0.2979 - val_accuracy: 0.8724\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2379 - accuracy: 0.8946 - val_loss: 0.3158 - val_accuracy: 0.8649\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2364 - accuracy: 0.8965 - val_loss: 0.3176 - val_accuracy: 0.8616\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2350 - accuracy: 0.8971 - val_loss: 0.3064 - val_accuracy: 0.8701\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2322 - accuracy: 0.8975 - val_loss: 0.3001 - val_accuracy: 0.8709\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2286 - accuracy: 0.8997 - val_loss: 0.2976 - val_accuracy: 0.8744\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2266 - accuracy: 0.9000 - val_loss: 0.2993 - val_accuracy: 0.8725\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2252 - accuracy: 0.9020 - val_loss: 0.3025 - val_accuracy: 0.8719\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2227 - accuracy: 0.9017 - val_loss: 0.3144 - val_accuracy: 0.8643\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2209 - accuracy: 0.9028 - val_loss: 0.3065 - val_accuracy: 0.8718\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2163 - accuracy: 0.9056 - val_loss: 0.3178 - val_accuracy: 0.8724\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2178 - accuracy: 0.9046 - val_loss: 0.2969 - val_accuracy: 0.8745\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2152 - accuracy: 0.9064 - val_loss: 0.3091 - val_accuracy: 0.8693\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2109 - accuracy: 0.9082 - val_loss: 0.2965 - val_accuracy: 0.8791\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2096 - accuracy: 0.9085 - val_loss: 0.3024 - val_accuracy: 0.8750\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2084 - accuracy: 0.9099 - val_loss: 0.2981 - val_accuracy: 0.8789\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2057 - accuracy: 0.9099 - val_loss: 0.3016 - val_accuracy: 0.8807\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2029 - accuracy: 0.9133 - val_loss: 0.2936 - val_accuracy: 0.8856\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.2031 - accuracy: 0.9120 - val_loss: 0.2924 - val_accuracy: 0.8831\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1993 - accuracy: 0.9130 - val_loss: 0.3177 - val_accuracy: 0.8730\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1991 - accuracy: 0.9146 - val_loss: 0.2928 - val_accuracy: 0.8831\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1959 - accuracy: 0.9161 - val_loss: 0.2990 - val_accuracy: 0.8827\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1948 - accuracy: 0.9161 - val_loss: 0.3058 - val_accuracy: 0.8836\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1920 - accuracy: 0.9173 - val_loss: 0.3089 - val_accuracy: 0.8799\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1903 - accuracy: 0.9179 - val_loss: 0.2989 - val_accuracy: 0.8865\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1892 - accuracy: 0.9192 - val_loss: 0.3025 - val_accuracy: 0.8839\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1879 - accuracy: 0.9190 - val_loss: 0.3052 - val_accuracy: 0.8869\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1845 - accuracy: 0.9219 - val_loss: 0.3100 - val_accuracy: 0.8777\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.1836 - accuracy: 0.9214 - val_loss: 0.2971 - val_accuracy: 0.8857\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,  # Split a portion of training data for validation\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 512)               6656      \n",
      "                                                                 \n",
      " p_re_lu_1 (PReLU)           (None, 512)               512       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " p_re_lu_2 (PReLU)           (None, 256)               256       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " p_re_lu_3 (PReLU)           (None, 128)               128       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171905 (671.50 KB)\n",
      "Trainable params: 171905 (671.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.4677 - accuracy: 0.7804 - val_loss: 0.4478 - val_accuracy: 0.7904\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4392 - accuracy: 0.7970 - val_loss: 0.4424 - val_accuracy: 0.7928\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4296 - accuracy: 0.8011 - val_loss: 0.4261 - val_accuracy: 0.8023\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4216 - accuracy: 0.8045 - val_loss: 0.4172 - val_accuracy: 0.8080\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4162 - accuracy: 0.8061 - val_loss: 0.4174 - val_accuracy: 0.8048\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4107 - accuracy: 0.8095 - val_loss: 0.4079 - val_accuracy: 0.8122\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4068 - accuracy: 0.8112 - val_loss: 0.4090 - val_accuracy: 0.8135\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4029 - accuracy: 0.8130 - val_loss: 0.4043 - val_accuracy: 0.8134\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3992 - accuracy: 0.8144 - val_loss: 0.3964 - val_accuracy: 0.8165\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3954 - accuracy: 0.8169 - val_loss: 0.3987 - val_accuracy: 0.8167\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3924 - accuracy: 0.8176 - val_loss: 0.3951 - val_accuracy: 0.8158\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.3884 - accuracy: 0.8197 - val_loss: 0.3936 - val_accuracy: 0.8210\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3854 - accuracy: 0.8203 - val_loss: 0.3929 - val_accuracy: 0.8174\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 22s 9ms/step - loss: 0.3834 - accuracy: 0.8224 - val_loss: 0.3855 - val_accuracy: 0.8213\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3806 - accuracy: 0.8230 - val_loss: 0.3826 - val_accuracy: 0.8245\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3776 - accuracy: 0.8248 - val_loss: 0.3730 - val_accuracy: 0.8272\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3744 - accuracy: 0.8268 - val_loss: 0.3924 - val_accuracy: 0.8229\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3709 - accuracy: 0.8272 - val_loss: 0.3733 - val_accuracy: 0.8295\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3673 - accuracy: 0.8293 - val_loss: 0.3735 - val_accuracy: 0.8259\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3641 - accuracy: 0.8308 - val_loss: 0.3673 - val_accuracy: 0.8313\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3600 - accuracy: 0.8341 - val_loss: 0.3682 - val_accuracy: 0.8314\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3569 - accuracy: 0.8350 - val_loss: 0.3773 - val_accuracy: 0.8310\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3521 - accuracy: 0.8375 - val_loss: 0.3735 - val_accuracy: 0.8300\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3495 - accuracy: 0.8385 - val_loss: 0.3635 - val_accuracy: 0.8345\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3470 - accuracy: 0.8396 - val_loss: 0.3646 - val_accuracy: 0.8314\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3427 - accuracy: 0.8435 - val_loss: 0.3565 - val_accuracy: 0.8378\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3391 - accuracy: 0.8447 - val_loss: 0.3586 - val_accuracy: 0.8358\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3344 - accuracy: 0.8467 - val_loss: 0.3499 - val_accuracy: 0.8431\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.3325 - accuracy: 0.8483 - val_loss: 0.3523 - val_accuracy: 0.8380\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.3280 - accuracy: 0.8507 - val_loss: 0.3511 - val_accuracy: 0.8410\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.3253 - accuracy: 0.8516 - val_loss: 0.3476 - val_accuracy: 0.8398\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3210 - accuracy: 0.8536 - val_loss: 0.3531 - val_accuracy: 0.8372\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3182 - accuracy: 0.8552 - val_loss: 0.3425 - val_accuracy: 0.8438\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3142 - accuracy: 0.8576 - val_loss: 0.3477 - val_accuracy: 0.8413\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3094 - accuracy: 0.8610 - val_loss: 0.3453 - val_accuracy: 0.8413\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 20s 8ms/step - loss: 0.3068 - accuracy: 0.8605 - val_loss: 0.3361 - val_accuracy: 0.8500\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3014 - accuracy: 0.8649 - val_loss: 0.3401 - val_accuracy: 0.8461\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2994 - accuracy: 0.8643 - val_loss: 0.3363 - val_accuracy: 0.8456\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2939 - accuracy: 0.8673 - val_loss: 0.3396 - val_accuracy: 0.8513\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2919 - accuracy: 0.8682 - val_loss: 0.3297 - val_accuracy: 0.8537\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2881 - accuracy: 0.8717 - val_loss: 0.3257 - val_accuracy: 0.8552\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2848 - accuracy: 0.8726 - val_loss: 0.3300 - val_accuracy: 0.8547\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2798 - accuracy: 0.8747 - val_loss: 0.3261 - val_accuracy: 0.8518\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2767 - accuracy: 0.8763 - val_loss: 0.3231 - val_accuracy: 0.8572\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 21s 9ms/step - loss: 0.2728 - accuracy: 0.8782 - val_loss: 0.3256 - val_accuracy: 0.8561\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.2680 - accuracy: 0.8806 - val_loss: 0.3257 - val_accuracy: 0.8568\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2643 - accuracy: 0.8816 - val_loss: 0.3220 - val_accuracy: 0.8604\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2613 - accuracy: 0.8842 - val_loss: 0.3247 - val_accuracy: 0.8584\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2553 - accuracy: 0.8870 - val_loss: 0.3114 - val_accuracy: 0.8655\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2535 - accuracy: 0.8883 - val_loss: 0.3070 - val_accuracy: 0.8682\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2486 - accuracy: 0.8903 - val_loss: 0.3265 - val_accuracy: 0.8638\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2461 - accuracy: 0.8926 - val_loss: 0.3039 - val_accuracy: 0.8687\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2432 - accuracy: 0.8931 - val_loss: 0.3116 - val_accuracy: 0.8663\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2394 - accuracy: 0.8957 - val_loss: 0.3063 - val_accuracy: 0.8677\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2343 - accuracy: 0.8983 - val_loss: 0.3102 - val_accuracy: 0.8673\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2312 - accuracy: 0.8993 - val_loss: 0.3124 - val_accuracy: 0.8650\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2264 - accuracy: 0.9022 - val_loss: 0.2951 - val_accuracy: 0.8765\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2253 - accuracy: 0.9015 - val_loss: 0.3069 - val_accuracy: 0.8701\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2202 - accuracy: 0.9045 - val_loss: 0.3035 - val_accuracy: 0.8771\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 20s 8ms/step - loss: 0.2197 - accuracy: 0.9041 - val_loss: 0.3080 - val_accuracy: 0.8749\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 20s 8ms/step - loss: 0.2142 - accuracy: 0.9081 - val_loss: 0.3137 - val_accuracy: 0.8771\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.2111 - accuracy: 0.9088 - val_loss: 0.3006 - val_accuracy: 0.8751\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2114 - accuracy: 0.9088 - val_loss: 0.2841 - val_accuracy: 0.8843\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.2057 - accuracy: 0.9119 - val_loss: 0.3154 - val_accuracy: 0.8698\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 21s 9ms/step - loss: 0.2042 - accuracy: 0.9129 - val_loss: 0.3086 - val_accuracy: 0.8738\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.2005 - accuracy: 0.9138 - val_loss: 0.3041 - val_accuracy: 0.8806\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.1975 - accuracy: 0.9159 - val_loss: 0.2952 - val_accuracy: 0.8808\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.1924 - accuracy: 0.9186 - val_loss: 0.2992 - val_accuracy: 0.8828\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.1896 - accuracy: 0.9192 - val_loss: 0.3151 - val_accuracy: 0.8735\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.1872 - accuracy: 0.9212 - val_loss: 0.2982 - val_accuracy: 0.8785\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.1850 - accuracy: 0.9211 - val_loss: 0.2906 - val_accuracy: 0.8834\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.1820 - accuracy: 0.9232 - val_loss: 0.2981 - val_accuracy: 0.8845\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.1784 - accuracy: 0.9241 - val_loss: 0.3017 - val_accuracy: 0.8862\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.PReLU())  # Use PReLU directly, no need to define it separately\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.PReLU())\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.PReLU())\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 512)               12288     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 176641 (690.00 KB)\n",
      "Trainable params: 176641 (690.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.4513 - accuracy: 0.7914 - val_loss: 0.4488 - val_accuracy: 0.7872\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4407 - accuracy: 0.7961 - val_loss: 0.4304 - val_accuracy: 0.8028\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4220 - accuracy: 0.8055 - val_loss: 0.4185 - val_accuracy: 0.8070\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.4085 - accuracy: 0.8109 - val_loss: 0.4030 - val_accuracy: 0.8149\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4008 - accuracy: 0.8152 - val_loss: 0.3968 - val_accuracy: 0.8198\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3888 - accuracy: 0.8206 - val_loss: 0.3878 - val_accuracy: 0.8216\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3709 - accuracy: 0.8282 - val_loss: 0.3598 - val_accuracy: 0.8316\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.3590 - accuracy: 0.8344 - val_loss: 0.3534 - val_accuracy: 0.8377\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3527 - accuracy: 0.8365 - val_loss: 0.3531 - val_accuracy: 0.8391\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3477 - accuracy: 0.8405 - val_loss: 0.3389 - val_accuracy: 0.8448\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3427 - accuracy: 0.8410 - val_loss: 0.3417 - val_accuracy: 0.8428\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3386 - accuracy: 0.8423 - val_loss: 0.3354 - val_accuracy: 0.8484\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3356 - accuracy: 0.8443 - val_loss: 0.3505 - val_accuracy: 0.8398\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3307 - accuracy: 0.8471 - val_loss: 0.3488 - val_accuracy: 0.8411\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3277 - accuracy: 0.8501 - val_loss: 0.3305 - val_accuracy: 0.8479\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3254 - accuracy: 0.8489 - val_loss: 0.3339 - val_accuracy: 0.8480\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3200 - accuracy: 0.8532 - val_loss: 0.3283 - val_accuracy: 0.8494\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3160 - accuracy: 0.8547 - val_loss: 0.3128 - val_accuracy: 0.8593\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.3122 - accuracy: 0.8569 - val_loss: 0.3482 - val_accuracy: 0.8383\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3112 - accuracy: 0.8563 - val_loss: 0.3102 - val_accuracy: 0.8612\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3077 - accuracy: 0.8595 - val_loss: 0.3080 - val_accuracy: 0.8612\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3024 - accuracy: 0.8613 - val_loss: 0.3004 - val_accuracy: 0.8659\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3004 - accuracy: 0.8626 - val_loss: 0.3000 - val_accuracy: 0.8637\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2968 - accuracy: 0.8640 - val_loss: 0.3030 - val_accuracy: 0.8640\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2932 - accuracy: 0.8666 - val_loss: 0.3021 - val_accuracy: 0.8633\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2915 - accuracy: 0.8674 - val_loss: 0.2820 - val_accuracy: 0.8758\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.2862 - accuracy: 0.8712 - val_loss: 0.3013 - val_accuracy: 0.8628\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2805 - accuracy: 0.8735 - val_loss: 0.3122 - val_accuracy: 0.8597\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2760 - accuracy: 0.8766 - val_loss: 0.2945 - val_accuracy: 0.8657\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2707 - accuracy: 0.8784 - val_loss: 0.2734 - val_accuracy: 0.8785\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2660 - accuracy: 0.8825 - val_loss: 0.2818 - val_accuracy: 0.8744\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2617 - accuracy: 0.8842 - val_loss: 0.2665 - val_accuracy: 0.8845\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2542 - accuracy: 0.8883 - val_loss: 0.2696 - val_accuracy: 0.8811\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.2485 - accuracy: 0.8915 - val_loss: 0.2550 - val_accuracy: 0.8891\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2438 - accuracy: 0.8935 - val_loss: 0.2644 - val_accuracy: 0.8834\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2371 - accuracy: 0.8969 - val_loss: 0.2387 - val_accuracy: 0.8988\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.2286 - accuracy: 0.9026 - val_loss: 0.2479 - val_accuracy: 0.8957\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.2231 - accuracy: 0.9042 - val_loss: 0.2389 - val_accuracy: 0.8975\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2143 - accuracy: 0.9093 - val_loss: 0.2294 - val_accuracy: 0.9040\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.2093 - accuracy: 0.9111 - val_loss: 0.2217 - val_accuracy: 0.9075\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2034 - accuracy: 0.9138 - val_loss: 0.2307 - val_accuracy: 0.9008\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1955 - accuracy: 0.9189 - val_loss: 0.2190 - val_accuracy: 0.9074\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1926 - accuracy: 0.9186 - val_loss: 0.2102 - val_accuracy: 0.9134\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1851 - accuracy: 0.9231 - val_loss: 0.2138 - val_accuracy: 0.9118\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1799 - accuracy: 0.9267 - val_loss: 0.2473 - val_accuracy: 0.9012\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1746 - accuracy: 0.9285 - val_loss: 0.2058 - val_accuracy: 0.9146\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1679 - accuracy: 0.9311 - val_loss: 0.2314 - val_accuracy: 0.9027\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1631 - accuracy: 0.9337 - val_loss: 0.1897 - val_accuracy: 0.9248\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1587 - accuracy: 0.9353 - val_loss: 0.1943 - val_accuracy: 0.9209\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1514 - accuracy: 0.9390 - val_loss: 0.2102 - val_accuracy: 0.9128\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1474 - accuracy: 0.9402 - val_loss: 0.1794 - val_accuracy: 0.9284\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1421 - accuracy: 0.9435 - val_loss: 0.1899 - val_accuracy: 0.9215\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1376 - accuracy: 0.9451 - val_loss: 0.1798 - val_accuracy: 0.9303\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1327 - accuracy: 0.9456 - val_loss: 0.2134 - val_accuracy: 0.9152\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.1295 - accuracy: 0.9480 - val_loss: 0.1715 - val_accuracy: 0.9334\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.1285 - accuracy: 0.9488 - val_loss: 0.1614 - val_accuracy: 0.9401\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 18s 8ms/step - loss: 0.1189 - accuracy: 0.9535 - val_loss: 0.1655 - val_accuracy: 0.9330\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1143 - accuracy: 0.9559 - val_loss: 0.1629 - val_accuracy: 0.9398\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 19s 8ms/step - loss: 0.1118 - accuracy: 0.9562 - val_loss: 0.1786 - val_accuracy: 0.9321\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.1073 - accuracy: 0.9588 - val_loss: 0.1593 - val_accuracy: 0.9411\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1050 - accuracy: 0.9600 - val_loss: 0.1621 - val_accuracy: 0.9372\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.1036 - accuracy: 0.9597 - val_loss: 0.1541 - val_accuracy: 0.9427\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0997 - accuracy: 0.9616 - val_loss: 0.1652 - val_accuracy: 0.9373\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0946 - accuracy: 0.9639 - val_loss: 0.1777 - val_accuracy: 0.9349\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0912 - accuracy: 0.9655 - val_loss: 0.1597 - val_accuracy: 0.9408\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.0876 - accuracy: 0.9670 - val_loss: 0.1541 - val_accuracy: 0.9448\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0873 - accuracy: 0.9661 - val_loss: 0.1645 - val_accuracy: 0.9390\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0827 - accuracy: 0.9682 - val_loss: 0.1724 - val_accuracy: 0.9400\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0797 - accuracy: 0.9695 - val_loss: 0.1633 - val_accuracy: 0.9437\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0791 - accuracy: 0.9701 - val_loss: 0.1532 - val_accuracy: 0.9443\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0759 - accuracy: 0.9717 - val_loss: 0.1538 - val_accuracy: 0.9488\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.0750 - accuracy: 0.9718 - val_loss: 0.1466 - val_accuracy: 0.9493\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.0707 - accuracy: 0.9738 - val_loss: 0.1450 - val_accuracy: 0.9500\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0681 - accuracy: 0.9750 - val_loss: 0.1574 - val_accuracy: 0.9448\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0647 - accuracy: 0.9761 - val_loss: 0.1575 - val_accuracy: 0.9473\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0695 - accuracy: 0.9748 - val_loss: 0.1567 - val_accuracy: 0.9479\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0642 - accuracy: 0.9758 - val_loss: 0.1757 - val_accuracy: 0.9427\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0605 - accuracy: 0.9774 - val_loss: 0.1558 - val_accuracy: 0.9480\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0610 - accuracy: 0.9768 - val_loss: 0.1488 - val_accuracy: 0.9506\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0605 - accuracy: 0.9772 - val_loss: 0.1487 - val_accuracy: 0.9506\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0559 - accuracy: 0.9791 - val_loss: 0.1537 - val_accuracy: 0.9504\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 0.1634 - val_accuracy: 0.9488\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.0547 - accuracy: 0.9801 - val_loss: 0.1597 - val_accuracy: 0.9452\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(23,)))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('model')\n",
    "import joblib\n",
    "# Save the StandardScaler\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.6795 - accuracy: 0.6596 - val_loss: 0.6605 - val_accuracy: 0.6858\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.5995 - accuracy: 0.7595 - val_loss: 0.5881 - val_accuracy: 0.6431\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.5013 - accuracy: 0.7668 - val_loss: 0.4906 - val_accuracy: 0.7668\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4935 - accuracy: 0.7675 - val_loss: 0.8438 - val_accuracy: 0.5431\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4929 - accuracy: 0.7675 - val_loss: 0.6499 - val_accuracy: 0.6419\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4918 - accuracy: 0.7685 - val_loss: 0.4895 - val_accuracy: 0.7734\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4893 - accuracy: 0.7704 - val_loss: 0.5013 - val_accuracy: 0.7731\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4895 - accuracy: 0.7700 - val_loss: 0.4864 - val_accuracy: 0.7722\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4887 - accuracy: 0.7731 - val_loss: 0.4935 - val_accuracy: 0.7628\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4883 - accuracy: 0.7722 - val_loss: 0.4880 - val_accuracy: 0.7736\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4875 - accuracy: 0.7715 - val_loss: 0.5407 - val_accuracy: 0.7231\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4870 - accuracy: 0.7733 - val_loss: 0.4866 - val_accuracy: 0.7693\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4868 - accuracy: 0.7718 - val_loss: 0.5101 - val_accuracy: 0.7728\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4860 - accuracy: 0.7723 - val_loss: 0.5089 - val_accuracy: 0.7734\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4853 - accuracy: 0.7731 - val_loss: 0.4981 - val_accuracy: 0.7575\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4849 - accuracy: 0.7740 - val_loss: 0.5232 - val_accuracy: 0.7363\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4850 - accuracy: 0.7728 - val_loss: 0.5022 - val_accuracy: 0.7748\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4846 - accuracy: 0.7732 - val_loss: 0.4841 - val_accuracy: 0.7740\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4846 - accuracy: 0.7745 - val_loss: 0.5463 - val_accuracy: 0.7584\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4834 - accuracy: 0.7745 - val_loss: 0.5077 - val_accuracy: 0.7735\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4837 - accuracy: 0.7740 - val_loss: 0.4962 - val_accuracy: 0.7766\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4836 - accuracy: 0.7735 - val_loss: 0.4893 - val_accuracy: 0.7666\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4831 - accuracy: 0.7743 - val_loss: 0.5087 - val_accuracy: 0.7487\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4834 - accuracy: 0.7743 - val_loss: 0.5623 - val_accuracy: 0.7071\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4828 - accuracy: 0.7748 - val_loss: 0.4941 - val_accuracy: 0.7611\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4825 - accuracy: 0.7750 - val_loss: 0.4899 - val_accuracy: 0.7769\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4821 - accuracy: 0.7758 - val_loss: 0.4825 - val_accuracy: 0.7727\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4822 - accuracy: 0.7748 - val_loss: 0.4821 - val_accuracy: 0.7742\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4822 - accuracy: 0.7755 - val_loss: 0.5070 - val_accuracy: 0.7738\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4819 - accuracy: 0.7749 - val_loss: 0.4876 - val_accuracy: 0.7771\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4817 - accuracy: 0.7750 - val_loss: 0.5023 - val_accuracy: 0.7547\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4817 - accuracy: 0.7758 - val_loss: 0.4833 - val_accuracy: 0.7772\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4810 - accuracy: 0.7755 - val_loss: 0.4892 - val_accuracy: 0.7657\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4810 - accuracy: 0.7753 - val_loss: 0.5271 - val_accuracy: 0.7320\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4813 - accuracy: 0.7765 - val_loss: 0.5061 - val_accuracy: 0.7505\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4802 - accuracy: 0.7772 - val_loss: 0.4814 - val_accuracy: 0.7735\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4810 - accuracy: 0.7752 - val_loss: 0.4935 - val_accuracy: 0.7616\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4808 - accuracy: 0.7763 - val_loss: 0.4888 - val_accuracy: 0.7770\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4800 - accuracy: 0.7766 - val_loss: 0.5011 - val_accuracy: 0.7766\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4807 - accuracy: 0.7757 - val_loss: 0.4806 - val_accuracy: 0.7752\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4803 - accuracy: 0.7769 - val_loss: 0.4845 - val_accuracy: 0.7781\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4802 - accuracy: 0.7768 - val_loss: 0.5085 - val_accuracy: 0.7740\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4799 - accuracy: 0.7763 - val_loss: 0.4907 - val_accuracy: 0.7787\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4794 - accuracy: 0.7770 - val_loss: 0.4860 - val_accuracy: 0.7672\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4796 - accuracy: 0.7759 - val_loss: 0.4842 - val_accuracy: 0.7787\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4798 - accuracy: 0.7765 - val_loss: 0.5019 - val_accuracy: 0.7768\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4799 - accuracy: 0.7770 - val_loss: 0.4816 - val_accuracy: 0.7773\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4794 - accuracy: 0.7771 - val_loss: 0.5041 - val_accuracy: 0.7527\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4787 - accuracy: 0.7774 - val_loss: 0.5167 - val_accuracy: 0.7724\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4794 - accuracy: 0.7772 - val_loss: 0.4938 - val_accuracy: 0.7787\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Create an instance of the SGD optimizer\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # You can adjust the learning rate\n",
    "\n",
    "# Compile the model with SGD optimizer\n",
    "model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4799 - accuracy: 0.7769 - val_loss: 0.4727 - val_accuracy: 0.7797\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4634 - accuracy: 0.7858 - val_loss: 0.4586 - val_accuracy: 0.7837\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4595 - accuracy: 0.7878 - val_loss: 0.4549 - val_accuracy: 0.7882\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4562 - accuracy: 0.7897 - val_loss: 0.4535 - val_accuracy: 0.7906\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4532 - accuracy: 0.7909 - val_loss: 0.4524 - val_accuracy: 0.7879\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4490 - accuracy: 0.7931 - val_loss: 0.4558 - val_accuracy: 0.7875\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4415 - accuracy: 0.7963 - val_loss: 0.4386 - val_accuracy: 0.7933\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4364 - accuracy: 0.7970 - val_loss: 0.4322 - val_accuracy: 0.8004\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4337 - accuracy: 0.7973 - val_loss: 0.4377 - val_accuracy: 0.7976\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4321 - accuracy: 0.7997 - val_loss: 0.4324 - val_accuracy: 0.8004\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4311 - accuracy: 0.7994 - val_loss: 0.4338 - val_accuracy: 0.7967\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4276 - accuracy: 0.8008 - val_loss: 0.4321 - val_accuracy: 0.7986\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4250 - accuracy: 0.8016 - val_loss: 0.4260 - val_accuracy: 0.8027\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4230 - accuracy: 0.8044 - val_loss: 0.4200 - val_accuracy: 0.8043\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4210 - accuracy: 0.8033 - val_loss: 0.4201 - val_accuracy: 0.8067\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4172 - accuracy: 0.8052 - val_loss: 0.4265 - val_accuracy: 0.8009\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4136 - accuracy: 0.8074 - val_loss: 0.4141 - val_accuracy: 0.8084\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4104 - accuracy: 0.8092 - val_loss: 0.4146 - val_accuracy: 0.8075\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4067 - accuracy: 0.8102 - val_loss: 0.4091 - val_accuracy: 0.8113\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4051 - accuracy: 0.8118 - val_loss: 0.4089 - val_accuracy: 0.8096\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4030 - accuracy: 0.8138 - val_loss: 0.4106 - val_accuracy: 0.8086\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4007 - accuracy: 0.8145 - val_loss: 0.4021 - val_accuracy: 0.8141\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3987 - accuracy: 0.8147 - val_loss: 0.3997 - val_accuracy: 0.8145\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3965 - accuracy: 0.8155 - val_loss: 0.3989 - val_accuracy: 0.8185\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3945 - accuracy: 0.8162 - val_loss: 0.3992 - val_accuracy: 0.8153\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3921 - accuracy: 0.8166 - val_loss: 0.3925 - val_accuracy: 0.8191\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3894 - accuracy: 0.8191 - val_loss: 0.3916 - val_accuracy: 0.8189\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3874 - accuracy: 0.8201 - val_loss: 0.3862 - val_accuracy: 0.8227\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3860 - accuracy: 0.8213 - val_loss: 0.3938 - val_accuracy: 0.8183\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3848 - accuracy: 0.8224 - val_loss: 0.3890 - val_accuracy: 0.8205\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3824 - accuracy: 0.8217 - val_loss: 0.3935 - val_accuracy: 0.8194\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3817 - accuracy: 0.8228 - val_loss: 0.3832 - val_accuracy: 0.8213\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3795 - accuracy: 0.8231 - val_loss: 0.3841 - val_accuracy: 0.8210\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3779 - accuracy: 0.8244 - val_loss: 0.3817 - val_accuracy: 0.8251\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3781 - accuracy: 0.8248 - val_loss: 0.3794 - val_accuracy: 0.8254\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3744 - accuracy: 0.8259 - val_loss: 0.3741 - val_accuracy: 0.8255\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3735 - accuracy: 0.8266 - val_loss: 0.3798 - val_accuracy: 0.8243\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3720 - accuracy: 0.8283 - val_loss: 0.3727 - val_accuracy: 0.8293\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3714 - accuracy: 0.8273 - val_loss: 0.3708 - val_accuracy: 0.8295\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3687 - accuracy: 0.8281 - val_loss: 0.3778 - val_accuracy: 0.8244\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3687 - accuracy: 0.8287 - val_loss: 0.3714 - val_accuracy: 0.8296\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3666 - accuracy: 0.8294 - val_loss: 0.3811 - val_accuracy: 0.8271\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3661 - accuracy: 0.8304 - val_loss: 0.3675 - val_accuracy: 0.8304\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3640 - accuracy: 0.8310 - val_loss: 0.3687 - val_accuracy: 0.8297\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3613 - accuracy: 0.8323 - val_loss: 0.3650 - val_accuracy: 0.8310\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3604 - accuracy: 0.8328 - val_loss: 0.3808 - val_accuracy: 0.8277\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3588 - accuracy: 0.8336 - val_loss: 0.3626 - val_accuracy: 0.8318\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3579 - accuracy: 0.8336 - val_loss: 0.3662 - val_accuracy: 0.8288\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3560 - accuracy: 0.8345 - val_loss: 0.3656 - val_accuracy: 0.8327\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3549 - accuracy: 0.8352 - val_loss: 0.3628 - val_accuracy: 0.8327\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3532 - accuracy: 0.8354 - val_loss: 0.3642 - val_accuracy: 0.8329\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3513 - accuracy: 0.8377 - val_loss: 0.3592 - val_accuracy: 0.8331\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3497 - accuracy: 0.8373 - val_loss: 0.3698 - val_accuracy: 0.8315\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3480 - accuracy: 0.8384 - val_loss: 0.3607 - val_accuracy: 0.8324\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3462 - accuracy: 0.8403 - val_loss: 0.3649 - val_accuracy: 0.8301\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3450 - accuracy: 0.8388 - val_loss: 0.3522 - val_accuracy: 0.8386\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3419 - accuracy: 0.8424 - val_loss: 0.3529 - val_accuracy: 0.8345\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3406 - accuracy: 0.8419 - val_loss: 0.3515 - val_accuracy: 0.8369\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3389 - accuracy: 0.8429 - val_loss: 0.3618 - val_accuracy: 0.8324\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3364 - accuracy: 0.8461 - val_loss: 0.3522 - val_accuracy: 0.8382\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3347 - accuracy: 0.8463 - val_loss: 0.3518 - val_accuracy: 0.8399\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3333 - accuracy: 0.8472 - val_loss: 0.3534 - val_accuracy: 0.8336\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3307 - accuracy: 0.8472 - val_loss: 0.3486 - val_accuracy: 0.8419\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3293 - accuracy: 0.8488 - val_loss: 0.3461 - val_accuracy: 0.8402\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3272 - accuracy: 0.8500 - val_loss: 0.3520 - val_accuracy: 0.8435\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3253 - accuracy: 0.8496 - val_loss: 0.3440 - val_accuracy: 0.8434\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3226 - accuracy: 0.8519 - val_loss: 0.3503 - val_accuracy: 0.8386\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3212 - accuracy: 0.8522 - val_loss: 0.3469 - val_accuracy: 0.8434\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3172 - accuracy: 0.8545 - val_loss: 0.3422 - val_accuracy: 0.8431\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3152 - accuracy: 0.8559 - val_loss: 0.3388 - val_accuracy: 0.8461\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3123 - accuracy: 0.8562 - val_loss: 0.3340 - val_accuracy: 0.8470\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3103 - accuracy: 0.8583 - val_loss: 0.3310 - val_accuracy: 0.8481\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3080 - accuracy: 0.8593 - val_loss: 0.3417 - val_accuracy: 0.8464\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3053 - accuracy: 0.8614 - val_loss: 0.3371 - val_accuracy: 0.8481\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3020 - accuracy: 0.8635 - val_loss: 0.3349 - val_accuracy: 0.8500\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3004 - accuracy: 0.8632 - val_loss: 0.3335 - val_accuracy: 0.8466\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2959 - accuracy: 0.8653 - val_loss: 0.3314 - val_accuracy: 0.8482\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2931 - accuracy: 0.8666 - val_loss: 0.3318 - val_accuracy: 0.8515\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2912 - accuracy: 0.8690 - val_loss: 0.3290 - val_accuracy: 0.8517\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2874 - accuracy: 0.8696 - val_loss: 0.3239 - val_accuracy: 0.8531\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2846 - accuracy: 0.8718 - val_loss: 0.3207 - val_accuracy: 0.8556\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2829 - accuracy: 0.8718 - val_loss: 0.3186 - val_accuracy: 0.8562\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2797 - accuracy: 0.8728 - val_loss: 0.3234 - val_accuracy: 0.8557\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2754 - accuracy: 0.8772 - val_loss: 0.3252 - val_accuracy: 0.8574\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2731 - accuracy: 0.8772 - val_loss: 0.3304 - val_accuracy: 0.8533\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2692 - accuracy: 0.8787 - val_loss: 0.3183 - val_accuracy: 0.8589\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2663 - accuracy: 0.8805 - val_loss: 0.3213 - val_accuracy: 0.8547\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2639 - accuracy: 0.8815 - val_loss: 0.3249 - val_accuracy: 0.8584\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2606 - accuracy: 0.8835 - val_loss: 0.3105 - val_accuracy: 0.8628\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2583 - accuracy: 0.8847 - val_loss: 0.3121 - val_accuracy: 0.8609\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2555 - accuracy: 0.8858 - val_loss: 0.3244 - val_accuracy: 0.8548\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2506 - accuracy: 0.8897 - val_loss: 0.3184 - val_accuracy: 0.8639\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2490 - accuracy: 0.8900 - val_loss: 0.3135 - val_accuracy: 0.8653\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2449 - accuracy: 0.8913 - val_loss: 0.3079 - val_accuracy: 0.8659\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2419 - accuracy: 0.8928 - val_loss: 0.3174 - val_accuracy: 0.8612\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2387 - accuracy: 0.8943 - val_loss: 0.3066 - val_accuracy: 0.8705\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2372 - accuracy: 0.8947 - val_loss: 0.3100 - val_accuracy: 0.8703\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2351 - accuracy: 0.8972 - val_loss: 0.3277 - val_accuracy: 0.8590\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2320 - accuracy: 0.8989 - val_loss: 0.3200 - val_accuracy: 0.8659\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.2285 - accuracy: 0.9004 - val_loss: 0.3122 - val_accuracy: 0.8666\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='elu'))\n",
    "model.add(layers.Dense(256, activation='elu'))\n",
    "model.add(layers.Dense(128, activation='elu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')\n",
    "import joblib\n",
    "# Save the StandardScaler\n",
    "joblib.dump(scaler, 'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 64)                4224      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4289 (16.75 KB)\n",
      "Trainable params: 4289 (16.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.5558 - accuracy: 0.7300 - val_loss: 0.4753 - val_accuracy: 0.7829\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4633 - accuracy: 0.7834 - val_loss: 0.4555 - val_accuracy: 0.7907\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4502 - accuracy: 0.7901 - val_loss: 0.4487 - val_accuracy: 0.7863\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4450 - accuracy: 0.7928 - val_loss: 0.4441 - val_accuracy: 0.7894\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4395 - accuracy: 0.7957 - val_loss: 0.4392 - val_accuracy: 0.7977\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4368 - accuracy: 0.7964 - val_loss: 0.4357 - val_accuracy: 0.7985\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4306 - accuracy: 0.7994 - val_loss: 0.4482 - val_accuracy: 0.7957\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4275 - accuracy: 0.8011 - val_loss: 0.4319 - val_accuracy: 0.8049\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4263 - accuracy: 0.8011 - val_loss: 0.4276 - val_accuracy: 0.7982\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4229 - accuracy: 0.8022 - val_loss: 0.4291 - val_accuracy: 0.8006\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4205 - accuracy: 0.8037 - val_loss: 0.4302 - val_accuracy: 0.7974\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4187 - accuracy: 0.8044 - val_loss: 0.4236 - val_accuracy: 0.8053\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4176 - accuracy: 0.8052 - val_loss: 0.4261 - val_accuracy: 0.8006\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4157 - accuracy: 0.8055 - val_loss: 0.4161 - val_accuracy: 0.8077\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4140 - accuracy: 0.8078 - val_loss: 0.4165 - val_accuracy: 0.8086\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4119 - accuracy: 0.8072 - val_loss: 0.4155 - val_accuracy: 0.8091\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4102 - accuracy: 0.8085 - val_loss: 0.4173 - val_accuracy: 0.8056\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4087 - accuracy: 0.8105 - val_loss: 0.4148 - val_accuracy: 0.8065\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4084 - accuracy: 0.8104 - val_loss: 0.4176 - val_accuracy: 0.8001\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.4073 - accuracy: 0.8118 - val_loss: 0.4111 - val_accuracy: 0.8105\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4065 - accuracy: 0.8119 - val_loss: 0.4095 - val_accuracy: 0.8092\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4054 - accuracy: 0.8115 - val_loss: 0.4054 - val_accuracy: 0.8131\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4043 - accuracy: 0.8133 - val_loss: 0.4115 - val_accuracy: 0.8090\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4041 - accuracy: 0.8125 - val_loss: 0.4074 - val_accuracy: 0.8103\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.4028 - accuracy: 0.8133 - val_loss: 0.4148 - val_accuracy: 0.8070\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.4024 - accuracy: 0.8137 - val_loss: 0.4055 - val_accuracy: 0.8102\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4012 - accuracy: 0.8137 - val_loss: 0.4106 - val_accuracy: 0.8072\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4003 - accuracy: 0.8151 - val_loss: 0.4078 - val_accuracy: 0.8104\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.4003 - accuracy: 0.8128 - val_loss: 0.4039 - val_accuracy: 0.8138\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3989 - accuracy: 0.8142 - val_loss: 0.4028 - val_accuracy: 0.8135\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3994 - accuracy: 0.8153 - val_loss: 0.4051 - val_accuracy: 0.8105\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3968 - accuracy: 0.8154 - val_loss: 0.4054 - val_accuracy: 0.8126\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3955 - accuracy: 0.8169 - val_loss: 0.4150 - val_accuracy: 0.8035\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3963 - accuracy: 0.8155 - val_loss: 0.4048 - val_accuracy: 0.8111\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3948 - accuracy: 0.8176 - val_loss: 0.4019 - val_accuracy: 0.8122\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3935 - accuracy: 0.8176 - val_loss: 0.4006 - val_accuracy: 0.8151\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3948 - accuracy: 0.8172 - val_loss: 0.4075 - val_accuracy: 0.8138\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3929 - accuracy: 0.8186 - val_loss: 0.4042 - val_accuracy: 0.8094\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3928 - accuracy: 0.8171 - val_loss: 0.4186 - val_accuracy: 0.8067\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3921 - accuracy: 0.8192 - val_loss: 0.4214 - val_accuracy: 0.8020\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3919 - accuracy: 0.8183 - val_loss: 0.3958 - val_accuracy: 0.8180\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3911 - accuracy: 0.8192 - val_loss: 0.4043 - val_accuracy: 0.8162\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3898 - accuracy: 0.8198 - val_loss: 0.4003 - val_accuracy: 0.8155\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3894 - accuracy: 0.8191 - val_loss: 0.4005 - val_accuracy: 0.8124\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3894 - accuracy: 0.8201 - val_loss: 0.4011 - val_accuracy: 0.8134\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3899 - accuracy: 0.8192 - val_loss: 0.4003 - val_accuracy: 0.8138\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3884 - accuracy: 0.8200 - val_loss: 0.3995 - val_accuracy: 0.8161\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3873 - accuracy: 0.8211 - val_loss: 0.4025 - val_accuracy: 0.8156\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3872 - accuracy: 0.8211 - val_loss: 0.4051 - val_accuracy: 0.8112\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3875 - accuracy: 0.8203 - val_loss: 0.3974 - val_accuracy: 0.8175\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3874 - accuracy: 0.8199 - val_loss: 0.4005 - val_accuracy: 0.8134\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3860 - accuracy: 0.8219 - val_loss: 0.4003 - val_accuracy: 0.8152\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3857 - accuracy: 0.8209 - val_loss: 0.4024 - val_accuracy: 0.8144\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3855 - accuracy: 0.8224 - val_loss: 0.4018 - val_accuracy: 0.8125\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3838 - accuracy: 0.8233 - val_loss: 0.3986 - val_accuracy: 0.8150\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3832 - accuracy: 0.8232 - val_loss: 0.4000 - val_accuracy: 0.8127\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3845 - accuracy: 0.8222 - val_loss: 0.3981 - val_accuracy: 0.8200\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3832 - accuracy: 0.8220 - val_loss: 0.4061 - val_accuracy: 0.8107\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3825 - accuracy: 0.8225 - val_loss: 0.3979 - val_accuracy: 0.8179\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3823 - accuracy: 0.8241 - val_loss: 0.3966 - val_accuracy: 0.8165\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3819 - accuracy: 0.8243 - val_loss: 0.3967 - val_accuracy: 0.8144\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3814 - accuracy: 0.8236 - val_loss: 0.3976 - val_accuracy: 0.8140\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3807 - accuracy: 0.8232 - val_loss: 0.4012 - val_accuracy: 0.8124\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3795 - accuracy: 0.8246 - val_loss: 0.3911 - val_accuracy: 0.8177\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3802 - accuracy: 0.8236 - val_loss: 0.3962 - val_accuracy: 0.8160\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3797 - accuracy: 0.8245 - val_loss: 0.3973 - val_accuracy: 0.8171\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3787 - accuracy: 0.8235 - val_loss: 0.4039 - val_accuracy: 0.8091\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3790 - accuracy: 0.8244 - val_loss: 0.3948 - val_accuracy: 0.8167\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3782 - accuracy: 0.8246 - val_loss: 0.4107 - val_accuracy: 0.8128\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3783 - accuracy: 0.8251 - val_loss: 0.3966 - val_accuracy: 0.8179\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3766 - accuracy: 0.8266 - val_loss: 0.3963 - val_accuracy: 0.8176\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3776 - accuracy: 0.8245 - val_loss: 0.3911 - val_accuracy: 0.8181\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3758 - accuracy: 0.8257 - val_loss: 0.3901 - val_accuracy: 0.8195\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3768 - accuracy: 0.8253 - val_loss: 0.3993 - val_accuracy: 0.8150\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3772 - accuracy: 0.8261 - val_loss: 0.3990 - val_accuracy: 0.8152\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.3749 - accuracy: 0.8253 - val_loss: 0.3901 - val_accuracy: 0.8196\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3749 - accuracy: 0.8267 - val_loss: 0.3877 - val_accuracy: 0.8203\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3747 - accuracy: 0.8271 - val_loss: 0.3928 - val_accuracy: 0.8173\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3744 - accuracy: 0.8275 - val_loss: 0.3897 - val_accuracy: 0.8232\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3754 - accuracy: 0.8265 - val_loss: 0.4060 - val_accuracy: 0.8101\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3749 - accuracy: 0.8264 - val_loss: 0.3924 - val_accuracy: 0.8197\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.3729 - accuracy: 0.8274 - val_loss: 0.3867 - val_accuracy: 0.8211\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3738 - accuracy: 0.8279 - val_loss: 0.3920 - val_accuracy: 0.8182\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3732 - accuracy: 0.8280 - val_loss: 0.3893 - val_accuracy: 0.8206\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3725 - accuracy: 0.8270 - val_loss: 0.3896 - val_accuracy: 0.8212\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3721 - accuracy: 0.8265 - val_loss: 0.3897 - val_accuracy: 0.8205\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3720 - accuracy: 0.8284 - val_loss: 0.3930 - val_accuracy: 0.8222\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3717 - accuracy: 0.8283 - val_loss: 0.3886 - val_accuracy: 0.8216\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3717 - accuracy: 0.8284 - val_loss: 0.3899 - val_accuracy: 0.8203\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3702 - accuracy: 0.8299 - val_loss: 0.3892 - val_accuracy: 0.8203\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3715 - accuracy: 0.8270 - val_loss: 0.3876 - val_accuracy: 0.8243\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3729 - accuracy: 0.8273 - val_loss: 0.3946 - val_accuracy: 0.8159\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3736 - accuracy: 0.8279 - val_loss: 0.3872 - val_accuracy: 0.8217\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3706 - accuracy: 0.8287 - val_loss: 0.3889 - val_accuracy: 0.8231\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3701 - accuracy: 0.8287 - val_loss: 0.4004 - val_accuracy: 0.8150\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3698 - accuracy: 0.8293 - val_loss: 0.3924 - val_accuracy: 0.8176\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3704 - accuracy: 0.8293 - val_loss: 0.3855 - val_accuracy: 0.8215\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3683 - accuracy: 0.8297 - val_loss: 0.3907 - val_accuracy: 0.8204\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3682 - accuracy: 0.8292 - val_loss: 0.3871 - val_accuracy: 0.8214\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3683 - accuracy: 0.8295 - val_loss: 0.3893 - val_accuracy: 0.8232\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Assuming X_train is your training data with shape (num_samples, 12)\n",
    "# and y_train is your corresponding target variable\n",
    "\n",
    "# Reshape data for RNN (assuming a sequence length of 12)\n",
    "sequence_length = 12\n",
    "X_train_reshaped = tf.reshape(X_train, (-1, sequence_length, 1))\n",
    "\n",
    "# Define the RNN model\n",
    "model = models.Sequential()\n",
    "model.add(layers.SimpleRNN(64, input_shape=(sequence_length, 1), activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2974/2974 [==============================] - 38s 12ms/step - loss: 0.2070 - accuracy: 0.7213 - val_loss: 0.1509 - val_accuracy: 0.7825\n",
      "Epoch 2/100\n",
      "2974/2974 [==============================] - 37s 12ms/step - loss: 0.1536 - accuracy: 0.7825 - val_loss: 0.1445 - val_accuracy: 0.7914\n",
      "Epoch 3/100\n",
      "2974/2974 [==============================] - 38s 13ms/step - loss: 0.1475 - accuracy: 0.7916 - val_loss: 0.1487 - val_accuracy: 0.7870\n",
      "Epoch 4/100\n",
      "2974/2974 [==============================] - 36s 12ms/step - loss: 0.1447 - accuracy: 0.7934 - val_loss: 0.1354 - val_accuracy: 0.8065\n",
      "Epoch 5/100\n",
      "2974/2974 [==============================] - 36s 12ms/step - loss: 0.1423 - accuracy: 0.7971 - val_loss: 0.1346 - val_accuracy: 0.8067\n",
      "Epoch 6/100\n",
      "2974/2974 [==============================] - 37s 12ms/step - loss: 0.1417 - accuracy: 0.7990 - val_loss: 0.1349 - val_accuracy: 0.8075\n",
      "Epoch 7/100\n",
      "2974/2974 [==============================] - 36s 12ms/step - loss: 0.1411 - accuracy: 0.8005 - val_loss: 0.1418 - val_accuracy: 0.8023\n",
      "Epoch 8/100\n",
      "2974/2974 [==============================] - 36s 12ms/step - loss: 0.1402 - accuracy: 0.8008 - val_loss: 0.1396 - val_accuracy: 0.7980\n",
      "Epoch 9/100\n",
      "2974/2974 [==============================] - 37s 12ms/step - loss: 0.1397 - accuracy: 0.8030 - val_loss: 0.1328 - val_accuracy: 0.8114\n",
      "Epoch 10/100\n",
      "2974/2974 [==============================] - 37s 12ms/step - loss: 0.1394 - accuracy: 0.8026 - val_loss: 0.1367 - val_accuracy: 0.8066\n",
      "Epoch 11/100\n",
      "2974/2974 [==============================] - 37s 12ms/step - loss: 0.1398 - accuracy: 0.8011 - val_loss: 0.1300 - val_accuracy: 0.8140\n",
      "Epoch 12/100\n",
      "2974/2974 [==============================] - 37s 13ms/step - loss: 0.1391 - accuracy: 0.8027 - val_loss: 0.1360 - val_accuracy: 0.8043\n",
      "Epoch 13/100\n",
      "2974/2974 [==============================] - 37s 13ms/step - loss: 0.1381 - accuracy: 0.8053 - val_loss: 0.1349 - val_accuracy: 0.8106\n",
      "Epoch 14/100\n",
      "2974/2974 [==============================] - 37s 13ms/step - loss: 0.1384 - accuracy: 0.8050 - val_loss: 0.1322 - val_accuracy: 0.8133\n",
      "Epoch 15/100\n",
      "2974/2974 [==============================] - 37s 13ms/step - loss: 0.1387 - accuracy: 0.8037 - val_loss: 0.1351 - val_accuracy: 0.8079\n",
      "Epoch 16/100\n",
      "2974/2974 [==============================] - 37s 13ms/step - loss: 0.1379 - accuracy: 0.8055 - val_loss: 0.1306 - val_accuracy: 0.8132\n",
      "Epoch 17/100\n",
      "2974/2974 [==============================] - 38s 13ms/step - loss: 0.1380 - accuracy: 0.8060 - val_loss: 0.1337 - val_accuracy: 0.8081\n",
      "Epoch 18/100\n",
      "2974/2974 [==============================] - 38s 13ms/step - loss: 0.1378 - accuracy: 0.8046 - val_loss: 0.1367 - val_accuracy: 0.8043\n",
      "Epoch 19/100\n",
      "2974/2974 [==============================] - 40s 13ms/step - loss: 0.1377 - accuracy: 0.8044 - val_loss: 0.1334 - val_accuracy: 0.8113\n",
      "Epoch 20/100\n",
      "2974/2974 [==============================] - 41s 14ms/step - loss: 0.1384 - accuracy: 0.8044 - val_loss: 0.1315 - val_accuracy: 0.8132\n",
      "Epoch 21/100\n",
      "2974/2974 [==============================] - 43s 15ms/step - loss: 0.1374 - accuracy: 0.8062 - val_loss: 0.1355 - val_accuracy: 0.8076\n",
      "Epoch 22/100\n",
      "2974/2974 [==============================] - 46s 15ms/step - loss: 0.1379 - accuracy: 0.8046 - val_loss: 0.1309 - val_accuracy: 0.8136\n",
      "Epoch 23/100\n",
      "2974/2974 [==============================] - 46s 15ms/step - loss: 0.1379 - accuracy: 0.8066 - val_loss: 0.1360 - val_accuracy: 0.8078\n",
      "Epoch 24/100\n",
      "2974/2974 [==============================] - 46s 15ms/step - loss: 0.1371 - accuracy: 0.8069 - val_loss: 0.1315 - val_accuracy: 0.8134\n",
      "Epoch 25/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1370 - accuracy: 0.8073 - val_loss: 0.1322 - val_accuracy: 0.8133\n",
      "Epoch 26/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1372 - accuracy: 0.8066 - val_loss: 0.1313 - val_accuracy: 0.8153\n",
      "Epoch 27/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1372 - accuracy: 0.8058 - val_loss: 0.1328 - val_accuracy: 0.8117\n",
      "Epoch 28/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1372 - accuracy: 0.8061 - val_loss: 0.1353 - val_accuracy: 0.8063\n",
      "Epoch 29/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1364 - accuracy: 0.8080 - val_loss: 0.1306 - val_accuracy: 0.8127\n",
      "Epoch 30/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1373 - accuracy: 0.8062 - val_loss: 0.1308 - val_accuracy: 0.8155\n",
      "Epoch 31/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1374 - accuracy: 0.8052 - val_loss: 0.1319 - val_accuracy: 0.8131\n",
      "Epoch 32/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1372 - accuracy: 0.8059 - val_loss: 0.1370 - val_accuracy: 0.8037\n",
      "Epoch 33/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1378 - accuracy: 0.8057 - val_loss: 0.1376 - val_accuracy: 0.8090\n",
      "Epoch 34/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1383 - accuracy: 0.8046 - val_loss: 0.1316 - val_accuracy: 0.8123\n",
      "Epoch 35/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1369 - accuracy: 0.8063 - val_loss: 0.1297 - val_accuracy: 0.8159\n",
      "Epoch 36/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1373 - accuracy: 0.8050 - val_loss: 0.1362 - val_accuracy: 0.8070\n",
      "Epoch 37/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1376 - accuracy: 0.8063 - val_loss: 0.1318 - val_accuracy: 0.8145\n",
      "Epoch 38/100\n",
      "2974/2974 [==============================] - 42s 14ms/step - loss: 0.1383 - accuracy: 0.8057 - val_loss: 0.1321 - val_accuracy: 0.8115\n",
      "Epoch 39/100\n",
      "2974/2974 [==============================] - 43s 15ms/step - loss: 0.1376 - accuracy: 0.8050 - val_loss: 0.1318 - val_accuracy: 0.8140\n",
      "Epoch 40/100\n",
      "2974/2974 [==============================] - 42s 14ms/step - loss: 0.1376 - accuracy: 0.8050 - val_loss: 0.1423 - val_accuracy: 0.7933\n",
      "Epoch 41/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1375 - accuracy: 0.8049 - val_loss: 0.1310 - val_accuracy: 0.8126\n",
      "Epoch 42/100\n",
      "2974/2974 [==============================] - 43s 14ms/step - loss: 0.1376 - accuracy: 0.8048 - val_loss: 0.1331 - val_accuracy: 0.8112\n",
      "Epoch 43/100\n",
      "2974/2974 [==============================] - 42s 14ms/step - loss: 0.1386 - accuracy: 0.8034 - val_loss: 0.1339 - val_accuracy: 0.8116\n",
      "Epoch 44/100\n",
      "2974/2974 [==============================] - 43s 14ms/step - loss: 0.1385 - accuracy: 0.8035 - val_loss: 0.1317 - val_accuracy: 0.8115\n",
      "Epoch 45/100\n",
      "2974/2974 [==============================] - 43s 14ms/step - loss: 0.1378 - accuracy: 0.8045 - val_loss: 0.1315 - val_accuracy: 0.8128\n",
      "Epoch 46/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1390 - accuracy: 0.8023 - val_loss: 0.1343 - val_accuracy: 0.8086\n",
      "Epoch 47/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1380 - accuracy: 0.8047 - val_loss: 0.1329 - val_accuracy: 0.8130\n",
      "Epoch 48/100\n",
      "2974/2974 [==============================] - 43s 14ms/step - loss: 0.1386 - accuracy: 0.8035 - val_loss: 0.1417 - val_accuracy: 0.8004\n",
      "Epoch 49/100\n",
      "2974/2974 [==============================] - 43s 15ms/step - loss: 0.1378 - accuracy: 0.8057 - val_loss: 0.1308 - val_accuracy: 0.8149\n",
      "Epoch 50/100\n",
      "2974/2974 [==============================] - 42s 14ms/step - loss: 0.1380 - accuracy: 0.8045 - val_loss: 0.1347 - val_accuracy: 0.8068\n",
      "Epoch 51/100\n",
      "2974/2974 [==============================] - 43s 15ms/step - loss: 0.1379 - accuracy: 0.8044 - val_loss: 0.1358 - val_accuracy: 0.8054\n",
      "Epoch 52/100\n",
      "2974/2974 [==============================] - 59s 20ms/step - loss: 0.1378 - accuracy: 0.8053 - val_loss: 0.1327 - val_accuracy: 0.8125\n",
      "Epoch 53/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1388 - accuracy: 0.8035 - val_loss: 0.1328 - val_accuracy: 0.8090\n",
      "Epoch 54/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1382 - accuracy: 0.8042 - val_loss: 0.1342 - val_accuracy: 0.8105\n",
      "Epoch 55/100\n",
      "2974/2974 [==============================] - 48s 16ms/step - loss: 0.1382 - accuracy: 0.8044 - val_loss: 0.1337 - val_accuracy: 0.8087\n",
      "Epoch 56/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1378 - accuracy: 0.8055 - val_loss: 0.1353 - val_accuracy: 0.8077\n",
      "Epoch 57/100\n",
      "2974/2974 [==============================] - 45s 15ms/step - loss: 0.1383 - accuracy: 0.8048 - val_loss: 0.1339 - val_accuracy: 0.8107\n",
      "Epoch 58/100\n",
      "2974/2974 [==============================] - 46s 16ms/step - loss: 0.1383 - accuracy: 0.8047 - val_loss: 0.1347 - val_accuracy: 0.8086\n",
      "Epoch 59/100\n",
      "2974/2974 [==============================] - 45s 15ms/step - loss: 0.1375 - accuracy: 0.8046 - val_loss: 0.1329 - val_accuracy: 0.8107\n",
      "Epoch 60/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1378 - accuracy: 0.8054 - val_loss: 0.1327 - val_accuracy: 0.8134\n",
      "Epoch 61/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1383 - accuracy: 0.8048 - val_loss: 0.1320 - val_accuracy: 0.8132\n",
      "Epoch 62/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1383 - accuracy: 0.8036 - val_loss: 0.1332 - val_accuracy: 0.8088\n",
      "Epoch 63/100\n",
      "2974/2974 [==============================] - 48s 16ms/step - loss: 0.1372 - accuracy: 0.8050 - val_loss: 0.1326 - val_accuracy: 0.8090\n",
      "Epoch 64/100\n",
      "2974/2974 [==============================] - 49s 17ms/step - loss: 0.1370 - accuracy: 0.8074 - val_loss: 0.1318 - val_accuracy: 0.8104\n",
      "Epoch 65/100\n",
      "2974/2974 [==============================] - 50s 17ms/step - loss: 0.1378 - accuracy: 0.8044 - val_loss: 0.1347 - val_accuracy: 0.8069\n",
      "Epoch 66/100\n",
      "2974/2974 [==============================] - 46s 15ms/step - loss: 0.1376 - accuracy: 0.8050 - val_loss: 0.1324 - val_accuracy: 0.8149\n",
      "Epoch 67/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1375 - accuracy: 0.8060 - val_loss: 0.1330 - val_accuracy: 0.8119\n",
      "Epoch 68/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1378 - accuracy: 0.8054 - val_loss: 0.1317 - val_accuracy: 0.8129\n",
      "Epoch 69/100\n",
      "2974/2974 [==============================] - 45s 15ms/step - loss: 0.1381 - accuracy: 0.8046 - val_loss: 0.1298 - val_accuracy: 0.8153\n",
      "Epoch 70/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1371 - accuracy: 0.8059 - val_loss: 0.1352 - val_accuracy: 0.8065\n",
      "Epoch 71/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1371 - accuracy: 0.8052 - val_loss: 0.1315 - val_accuracy: 0.8130\n",
      "Epoch 72/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1378 - accuracy: 0.8053 - val_loss: 0.1318 - val_accuracy: 0.8111\n",
      "Epoch 73/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1378 - accuracy: 0.8040 - val_loss: 0.1357 - val_accuracy: 0.8056\n",
      "Epoch 74/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1380 - accuracy: 0.8051 - val_loss: 0.1320 - val_accuracy: 0.8124\n",
      "Epoch 75/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1382 - accuracy: 0.8045 - val_loss: 0.1348 - val_accuracy: 0.8084\n",
      "Epoch 76/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1379 - accuracy: 0.8051 - val_loss: 0.1342 - val_accuracy: 0.8102\n",
      "Epoch 77/100\n",
      "2974/2974 [==============================] - 43s 14ms/step - loss: 0.1381 - accuracy: 0.8050 - val_loss: 0.1372 - val_accuracy: 0.8052\n",
      "Epoch 78/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1390 - accuracy: 0.8033 - val_loss: 0.1373 - val_accuracy: 0.7998\n",
      "Epoch 79/100\n",
      "2974/2974 [==============================] - 49s 16ms/step - loss: 0.1384 - accuracy: 0.8042 - val_loss: 0.1323 - val_accuracy: 0.8121\n",
      "Epoch 80/100\n",
      "2974/2974 [==============================] - 44s 15ms/step - loss: 0.1384 - accuracy: 0.8033 - val_loss: 0.1334 - val_accuracy: 0.8093\n",
      "Epoch 81/100\n",
      "2974/2974 [==============================] - 47s 16ms/step - loss: 0.1373 - accuracy: 0.8059 - val_loss: 0.1321 - val_accuracy: 0.8108\n",
      "Epoch 82/100\n",
      "2342/2974 [======================>.......] - ETA: 8s - loss: 0.1383 - accuracy: 0.8042"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chand\\OneDrive\\Desktop\\Gait Project\\Code\\modeltesting.ipynb Cell 25\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X41sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m regressor\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X41sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# Fitting the RNN to the Training set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X41sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m history \u001b[39m=\u001b[39m regressor\u001b[39m.\u001b[39;49mfit(X_train_scaled, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test_scaled, y_test))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Dropout\n",
    "\n",
    "# Assuming X and y are your features and target variable\n",
    "# Replace this with your actual data\n",
    "# X, y = ...\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Apply Min-Max Scaling to the feature variables\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape X_train and X_test for RNN input (assuming 1D time series)\n",
    "X_train_scaled = np.reshape(X_train_scaled, (X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_test_scaled = np.reshape(X_test_scaled, (X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Initialize RNN:\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first RNN layer and some Dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation='tanh', return_sequences=True, input_shape=(X_train_scaled.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the second RNN layer and some Dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation='tanh', return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the third RNN layer and some Dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation='tanh', return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the fourth RNN layer and some Dropout regularization\n",
    "regressor.add(SimpleRNN(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compile the RNN\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "history = regressor.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_data=(X_test_scaled, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4815 - accuracy: 0.7764 - val_loss: 0.4715 - val_accuracy: 0.7813\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.4655 - accuracy: 0.7865 - val_loss: 0.4701 - val_accuracy: 0.7820\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4600 - accuracy: 0.7875 - val_loss: 0.4632 - val_accuracy: 0.7848\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4566 - accuracy: 0.7889 - val_loss: 0.4704 - val_accuracy: 0.7809\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4532 - accuracy: 0.7907 - val_loss: 0.4529 - val_accuracy: 0.7936\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4491 - accuracy: 0.7917 - val_loss: 0.4499 - val_accuracy: 0.7945\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4433 - accuracy: 0.7944 - val_loss: 0.4504 - val_accuracy: 0.7929\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4398 - accuracy: 0.7959 - val_loss: 0.4585 - val_accuracy: 0.7874\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4371 - accuracy: 0.7966 - val_loss: 0.4381 - val_accuracy: 0.7977\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4330 - accuracy: 0.7989 - val_loss: 0.4301 - val_accuracy: 0.8003\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4304 - accuracy: 0.7996 - val_loss: 0.4347 - val_accuracy: 0.7984\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4286 - accuracy: 0.8006 - val_loss: 0.4324 - val_accuracy: 0.8005\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4275 - accuracy: 0.8012 - val_loss: 0.4289 - val_accuracy: 0.8013\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.4254 - accuracy: 0.8005 - val_loss: 0.4272 - val_accuracy: 0.8019\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4236 - accuracy: 0.8018 - val_loss: 0.4233 - val_accuracy: 0.8024\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4215 - accuracy: 0.8049 - val_loss: 0.4220 - val_accuracy: 0.8032\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4187 - accuracy: 0.8046 - val_loss: 0.4197 - val_accuracy: 0.8061\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4159 - accuracy: 0.8060 - val_loss: 0.4136 - val_accuracy: 0.8083\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4123 - accuracy: 0.8071 - val_loss: 0.4125 - val_accuracy: 0.8066\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4100 - accuracy: 0.8081 - val_loss: 0.4189 - val_accuracy: 0.8071\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4086 - accuracy: 0.8105 - val_loss: 0.4109 - val_accuracy: 0.8071\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4060 - accuracy: 0.8109 - val_loss: 0.4133 - val_accuracy: 0.8092\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4034 - accuracy: 0.8120 - val_loss: 0.4076 - val_accuracy: 0.8118\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4018 - accuracy: 0.8116 - val_loss: 0.4020 - val_accuracy: 0.8141\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3990 - accuracy: 0.8147 - val_loss: 0.4051 - val_accuracy: 0.8142\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3950 - accuracy: 0.8171 - val_loss: 0.3981 - val_accuracy: 0.8162\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3922 - accuracy: 0.8176 - val_loss: 0.4028 - val_accuracy: 0.8169\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3892 - accuracy: 0.8186 - val_loss: 0.3895 - val_accuracy: 0.8186\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3862 - accuracy: 0.8209 - val_loss: 0.3879 - val_accuracy: 0.8215\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3835 - accuracy: 0.8219 - val_loss: 0.3874 - val_accuracy: 0.8235\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3805 - accuracy: 0.8231 - val_loss: 0.3824 - val_accuracy: 0.8248\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3786 - accuracy: 0.8238 - val_loss: 0.3818 - val_accuracy: 0.8260\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3763 - accuracy: 0.8253 - val_loss: 0.4026 - val_accuracy: 0.8144\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3743 - accuracy: 0.8251 - val_loss: 0.3776 - val_accuracy: 0.8242\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3716 - accuracy: 0.8266 - val_loss: 0.3786 - val_accuracy: 0.8252\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.3692 - accuracy: 0.8284 - val_loss: 0.3712 - val_accuracy: 0.8304\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.3665 - accuracy: 0.8298 - val_loss: 0.3874 - val_accuracy: 0.8244\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3637 - accuracy: 0.8305 - val_loss: 0.3683 - val_accuracy: 0.8291\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3620 - accuracy: 0.8310 - val_loss: 0.3650 - val_accuracy: 0.8332\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3598 - accuracy: 0.8333 - val_loss: 0.3642 - val_accuracy: 0.8320\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3566 - accuracy: 0.8337 - val_loss: 0.3623 - val_accuracy: 0.8319\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3547 - accuracy: 0.8354 - val_loss: 0.3629 - val_accuracy: 0.8338\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3532 - accuracy: 0.8357 - val_loss: 0.3652 - val_accuracy: 0.8318\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3501 - accuracy: 0.8380 - val_loss: 0.3712 - val_accuracy: 0.8288\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3465 - accuracy: 0.8400 - val_loss: 0.3558 - val_accuracy: 0.8370\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3429 - accuracy: 0.8420 - val_loss: 0.3694 - val_accuracy: 0.8306\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3406 - accuracy: 0.8418 - val_loss: 0.3556 - val_accuracy: 0.8384\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3370 - accuracy: 0.8448 - val_loss: 0.3526 - val_accuracy: 0.8393\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3338 - accuracy: 0.8458 - val_loss: 0.3543 - val_accuracy: 0.8363\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3311 - accuracy: 0.8470 - val_loss: 0.3416 - val_accuracy: 0.8426\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3272 - accuracy: 0.8486 - val_loss: 0.3452 - val_accuracy: 0.8439\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3237 - accuracy: 0.8505 - val_loss: 0.3462 - val_accuracy: 0.8386\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3216 - accuracy: 0.8533 - val_loss: 0.3484 - val_accuracy: 0.8409\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3176 - accuracy: 0.8540 - val_loss: 0.3442 - val_accuracy: 0.8416\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3142 - accuracy: 0.8571 - val_loss: 0.3401 - val_accuracy: 0.8432\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3123 - accuracy: 0.8559 - val_loss: 0.3359 - val_accuracy: 0.8435\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3071 - accuracy: 0.8596 - val_loss: 0.3367 - val_accuracy: 0.8462\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3057 - accuracy: 0.8616 - val_loss: 0.3324 - val_accuracy: 0.8496\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3022 - accuracy: 0.8609 - val_loss: 0.3408 - val_accuracy: 0.8477\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2983 - accuracy: 0.8643 - val_loss: 0.3365 - val_accuracy: 0.8473\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2958 - accuracy: 0.8643 - val_loss: 0.3295 - val_accuracy: 0.8524\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2927 - accuracy: 0.8665 - val_loss: 0.3282 - val_accuracy: 0.8523\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2885 - accuracy: 0.8689 - val_loss: 0.3225 - val_accuracy: 0.8570\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2870 - accuracy: 0.8703 - val_loss: 0.3356 - val_accuracy: 0.8475\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2837 - accuracy: 0.8711 - val_loss: 0.3315 - val_accuracy: 0.8504\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2812 - accuracy: 0.8724 - val_loss: 0.3258 - val_accuracy: 0.8533\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2777 - accuracy: 0.8729 - val_loss: 0.3175 - val_accuracy: 0.8573\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2745 - accuracy: 0.8763 - val_loss: 0.3222 - val_accuracy: 0.8562\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.2726 - accuracy: 0.8770 - val_loss: 0.3190 - val_accuracy: 0.8603\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2699 - accuracy: 0.8777 - val_loss: 0.3254 - val_accuracy: 0.8565\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2670 - accuracy: 0.8791 - val_loss: 0.3224 - val_accuracy: 0.8580\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2647 - accuracy: 0.8814 - val_loss: 0.3137 - val_accuracy: 0.8622\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2610 - accuracy: 0.8824 - val_loss: 0.3165 - val_accuracy: 0.8605\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2601 - accuracy: 0.8827 - val_loss: 0.3133 - val_accuracy: 0.8607\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2564 - accuracy: 0.8851 - val_loss: 0.3326 - val_accuracy: 0.8517\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2528 - accuracy: 0.8872 - val_loss: 0.3236 - val_accuracy: 0.8612\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2517 - accuracy: 0.8877 - val_loss: 0.3056 - val_accuracy: 0.8677\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2487 - accuracy: 0.8888 - val_loss: 0.3161 - val_accuracy: 0.8626\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2440 - accuracy: 0.8928 - val_loss: 0.3157 - val_accuracy: 0.8640\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2429 - accuracy: 0.8914 - val_loss: 0.3222 - val_accuracy: 0.8632\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2403 - accuracy: 0.8939 - val_loss: 0.3313 - val_accuracy: 0.8608\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2382 - accuracy: 0.8941 - val_loss: 0.3072 - val_accuracy: 0.8695\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2347 - accuracy: 0.8960 - val_loss: 0.3065 - val_accuracy: 0.8709\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2330 - accuracy: 0.8981 - val_loss: 0.3089 - val_accuracy: 0.8711\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2298 - accuracy: 0.8989 - val_loss: 0.3114 - val_accuracy: 0.8701\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2277 - accuracy: 0.8997 - val_loss: 0.3170 - val_accuracy: 0.8625\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2265 - accuracy: 0.8993 - val_loss: 0.3177 - val_accuracy: 0.8699\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the Swish activation function\n",
    "def swish_activation(x):\n",
    "    return x * tf.math.sigmoid(x)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation=swish_activation))\n",
    "model.add(layers.Dense(256, activation=swish_activation))\n",
    "model.add(layers.Dense(128, activation=swish_activation))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171009 (668.00 KB)\n",
      "Trainable params: 171009 (668.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6941 - accuracy: 0.4821 - val_loss: 0.6930 - val_accuracy: 0.5313\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.6920 - accuracy: 0.5033 - val_loss: 0.6910 - val_accuracy: 0.5019\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.6900 - accuracy: 0.5012 - val_loss: 0.6890 - val_accuracy: 0.5046\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 15s 7ms/step - loss: 0.6880 - accuracy: 0.5051 - val_loss: 0.6870 - val_accuracy: 0.5203\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.6859 - accuracy: 0.5358 - val_loss: 0.6850 - val_accuracy: 0.5583\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.6838 - accuracy: 0.5755 - val_loss: 0.6828 - val_accuracy: 0.6101\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.6815 - accuracy: 0.6341 - val_loss: 0.6805 - val_accuracy: 0.6553\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.6790 - accuracy: 0.6675 - val_loss: 0.6780 - val_accuracy: 0.6859\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.6763 - accuracy: 0.7045 - val_loss: 0.6752 - val_accuracy: 0.6996\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6734 - accuracy: 0.7117 - val_loss: 0.6721 - val_accuracy: 0.7207\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6701 - accuracy: 0.7317 - val_loss: 0.6687 - val_accuracy: 0.7311\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6664 - accuracy: 0.7348 - val_loss: 0.6649 - val_accuracy: 0.7382\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6623 - accuracy: 0.7454 - val_loss: 0.6606 - val_accuracy: 0.7450\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6576 - accuracy: 0.7492 - val_loss: 0.6558 - val_accuracy: 0.7504\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6524 - accuracy: 0.7542 - val_loss: 0.6503 - val_accuracy: 0.7535\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6465 - accuracy: 0.7570 - val_loss: 0.6442 - val_accuracy: 0.7577\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6399 - accuracy: 0.7597 - val_loss: 0.6374 - val_accuracy: 0.7603\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6325 - accuracy: 0.7628 - val_loss: 0.6298 - val_accuracy: 0.7612\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6244 - accuracy: 0.7630 - val_loss: 0.6215 - val_accuracy: 0.7652\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6156 - accuracy: 0.7658 - val_loss: 0.6125 - val_accuracy: 0.7656\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.6061 - accuracy: 0.7666 - val_loss: 0.6028 - val_accuracy: 0.7665\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.5959 - accuracy: 0.7671 - val_loss: 0.5927 - val_accuracy: 0.7665\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 18s 7ms/step - loss: 0.5854 - accuracy: 0.7694 - val_loss: 0.5822 - val_accuracy: 0.7667\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.5747 - accuracy: 0.7685 - val_loss: 0.5717 - val_accuracy: 0.7679\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.5640 - accuracy: 0.7702 - val_loss: 0.5613 - val_accuracy: 0.7671\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 6862s 3s/step - loss: 0.5536 - accuracy: 0.7699 - val_loss: 0.5513 - val_accuracy: 0.7673\n",
      "Epoch 27/100\n",
      "1840/2380 [======================>.......] - ETA: 3s - loss: 0.5446 - accuracy: 0.7712"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chand\\OneDrive\\Desktop\\Gait Project\\Code\\modeltesting.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     X_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X36sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer\n",
    "model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 256)               3328      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44545 (174.00 KB)\n",
      "Trainable params: 44545 (174.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4834 - accuracy: 0.7765 - val_loss: 0.4809 - val_accuracy: 0.7803\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4692 - accuracy: 0.7822 - val_loss: 0.4643 - val_accuracy: 0.7859\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4566 - accuracy: 0.7879 - val_loss: 0.4538 - val_accuracy: 0.7882\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4528 - accuracy: 0.7894 - val_loss: 0.4619 - val_accuracy: 0.7848\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4497 - accuracy: 0.7903 - val_loss: 0.4508 - val_accuracy: 0.7919\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4459 - accuracy: 0.7930 - val_loss: 0.4434 - val_accuracy: 0.7973\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4444 - accuracy: 0.7942 - val_loss: 0.4442 - val_accuracy: 0.7953\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4424 - accuracy: 0.7960 - val_loss: 0.4420 - val_accuracy: 0.7948\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4407 - accuracy: 0.7943 - val_loss: 0.4403 - val_accuracy: 0.7978\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4384 - accuracy: 0.7958 - val_loss: 0.4421 - val_accuracy: 0.7988\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4348 - accuracy: 0.7981 - val_loss: 0.4381 - val_accuracy: 0.7955\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4315 - accuracy: 0.7993 - val_loss: 0.4321 - val_accuracy: 0.8014\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4292 - accuracy: 0.8001 - val_loss: 0.4370 - val_accuracy: 0.7999\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4263 - accuracy: 0.8025 - val_loss: 0.4294 - val_accuracy: 0.8024\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4233 - accuracy: 0.8031 - val_loss: 0.4279 - val_accuracy: 0.8030\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4206 - accuracy: 0.8052 - val_loss: 0.4260 - val_accuracy: 0.8024\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4183 - accuracy: 0.8061 - val_loss: 0.4195 - val_accuracy: 0.8097\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4167 - accuracy: 0.8072 - val_loss: 0.4241 - val_accuracy: 0.8044\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.4130 - accuracy: 0.8087 - val_loss: 0.4147 - val_accuracy: 0.8087\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.4117 - accuracy: 0.8098 - val_loss: 0.4155 - val_accuracy: 0.8088\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4095 - accuracy: 0.8103 - val_loss: 0.4208 - val_accuracy: 0.8053\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4077 - accuracy: 0.8112 - val_loss: 0.4095 - val_accuracy: 0.8113\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4056 - accuracy: 0.8120 - val_loss: 0.4292 - val_accuracy: 0.8057\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4044 - accuracy: 0.8138 - val_loss: 0.4099 - val_accuracy: 0.8125\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4033 - accuracy: 0.8140 - val_loss: 0.4085 - val_accuracy: 0.8128\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4001 - accuracy: 0.8164 - val_loss: 0.4124 - val_accuracy: 0.8097\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3992 - accuracy: 0.8155 - val_loss: 0.4117 - val_accuracy: 0.8107\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3955 - accuracy: 0.8172 - val_loss: 0.4036 - val_accuracy: 0.8148\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3944 - accuracy: 0.8183 - val_loss: 0.4104 - val_accuracy: 0.8088\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3923 - accuracy: 0.8196 - val_loss: 0.3941 - val_accuracy: 0.8198\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3896 - accuracy: 0.8202 - val_loss: 0.3968 - val_accuracy: 0.8173\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3870 - accuracy: 0.8209 - val_loss: 0.3978 - val_accuracy: 0.8165\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3852 - accuracy: 0.8230 - val_loss: 0.4002 - val_accuracy: 0.8161\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3838 - accuracy: 0.8238 - val_loss: 0.3879 - val_accuracy: 0.8216\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3825 - accuracy: 0.8244 - val_loss: 0.3914 - val_accuracy: 0.8227\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3810 - accuracy: 0.8253 - val_loss: 0.3883 - val_accuracy: 0.8250\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3791 - accuracy: 0.8259 - val_loss: 0.3868 - val_accuracy: 0.8223\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3774 - accuracy: 0.8265 - val_loss: 0.3896 - val_accuracy: 0.8223\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3761 - accuracy: 0.8282 - val_loss: 0.3850 - val_accuracy: 0.8226\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3741 - accuracy: 0.8282 - val_loss: 0.3832 - val_accuracy: 0.8235\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3729 - accuracy: 0.8291 - val_loss: 0.3830 - val_accuracy: 0.8259\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3713 - accuracy: 0.8297 - val_loss: 0.3790 - val_accuracy: 0.8258\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3682 - accuracy: 0.8320 - val_loss: 0.3834 - val_accuracy: 0.8243\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3670 - accuracy: 0.8312 - val_loss: 0.3785 - val_accuracy: 0.8274\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3655 - accuracy: 0.8330 - val_loss: 0.3743 - val_accuracy: 0.8308\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3637 - accuracy: 0.8341 - val_loss: 0.3777 - val_accuracy: 0.8271\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3618 - accuracy: 0.8360 - val_loss: 0.3833 - val_accuracy: 0.8255\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3595 - accuracy: 0.8367 - val_loss: 0.3849 - val_accuracy: 0.8246\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3573 - accuracy: 0.8368 - val_loss: 0.3718 - val_accuracy: 0.8290\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3556 - accuracy: 0.8379 - val_loss: 0.3753 - val_accuracy: 0.8273\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3542 - accuracy: 0.8383 - val_loss: 0.3683 - val_accuracy: 0.8301\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3515 - accuracy: 0.8413 - val_loss: 0.3700 - val_accuracy: 0.8278\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3502 - accuracy: 0.8411 - val_loss: 0.3680 - val_accuracy: 0.8303\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3480 - accuracy: 0.8417 - val_loss: 0.3714 - val_accuracy: 0.8275\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3459 - accuracy: 0.8427 - val_loss: 0.3657 - val_accuracy: 0.8342\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3443 - accuracy: 0.8439 - val_loss: 0.3621 - val_accuracy: 0.8321\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3421 - accuracy: 0.8452 - val_loss: 0.3578 - val_accuracy: 0.8357\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3396 - accuracy: 0.8457 - val_loss: 0.3567 - val_accuracy: 0.8366\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3383 - accuracy: 0.8462 - val_loss: 0.3532 - val_accuracy: 0.8392\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3369 - accuracy: 0.8475 - val_loss: 0.3508 - val_accuracy: 0.8391\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3340 - accuracy: 0.8485 - val_loss: 0.3558 - val_accuracy: 0.8391\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3327 - accuracy: 0.8498 - val_loss: 0.3525 - val_accuracy: 0.8382\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3303 - accuracy: 0.8499 - val_loss: 0.3572 - val_accuracy: 0.8381\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3292 - accuracy: 0.8504 - val_loss: 0.3502 - val_accuracy: 0.8394\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3263 - accuracy: 0.8521 - val_loss: 0.3506 - val_accuracy: 0.8389\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3253 - accuracy: 0.8525 - val_loss: 0.3589 - val_accuracy: 0.8346\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3220 - accuracy: 0.8532 - val_loss: 0.3454 - val_accuracy: 0.8428\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3210 - accuracy: 0.8553 - val_loss: 0.3626 - val_accuracy: 0.8328\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3201 - accuracy: 0.8557 - val_loss: 0.3481 - val_accuracy: 0.8431\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3174 - accuracy: 0.8570 - val_loss: 0.3521 - val_accuracy: 0.8411\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3164 - accuracy: 0.8578 - val_loss: 0.3611 - val_accuracy: 0.8369\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3143 - accuracy: 0.8585 - val_loss: 0.3670 - val_accuracy: 0.8348\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3126 - accuracy: 0.8597 - val_loss: 0.3442 - val_accuracy: 0.8445\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3107 - accuracy: 0.8600 - val_loss: 0.3419 - val_accuracy: 0.8467\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3097 - accuracy: 0.8611 - val_loss: 0.3475 - val_accuracy: 0.8452\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3076 - accuracy: 0.8610 - val_loss: 0.3438 - val_accuracy: 0.8438\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3061 - accuracy: 0.8620 - val_loss: 0.3464 - val_accuracy: 0.8456\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3046 - accuracy: 0.8626 - val_loss: 0.3455 - val_accuracy: 0.8457\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3024 - accuracy: 0.8659 - val_loss: 0.3418 - val_accuracy: 0.8461\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3005 - accuracy: 0.8654 - val_loss: 0.3361 - val_accuracy: 0.8517\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2988 - accuracy: 0.8667 - val_loss: 0.3419 - val_accuracy: 0.8463\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.2956 - accuracy: 0.8680 - val_loss: 0.3375 - val_accuracy: 0.8513\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2955 - accuracy: 0.8689 - val_loss: 0.3329 - val_accuracy: 0.8545\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2926 - accuracy: 0.8690 - val_loss: 0.3442 - val_accuracy: 0.8492\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2904 - accuracy: 0.8722 - val_loss: 0.3412 - val_accuracy: 0.8493\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2873 - accuracy: 0.8739 - val_loss: 0.3262 - val_accuracy: 0.8543\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2849 - accuracy: 0.8740 - val_loss: 0.3288 - val_accuracy: 0.8552\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2830 - accuracy: 0.8748 - val_loss: 0.3356 - val_accuracy: 0.8543\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2813 - accuracy: 0.8757 - val_loss: 0.3316 - val_accuracy: 0.8538\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2788 - accuracy: 0.8771 - val_loss: 0.3276 - val_accuracy: 0.8581\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2772 - accuracy: 0.8783 - val_loss: 0.3281 - val_accuracy: 0.8545\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2748 - accuracy: 0.8787 - val_loss: 0.3326 - val_accuracy: 0.8537\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2742 - accuracy: 0.8796 - val_loss: 0.3250 - val_accuracy: 0.8564\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2705 - accuracy: 0.8812 - val_loss: 0.3218 - val_accuracy: 0.8580\n",
      "Epoch 95/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2677 - accuracy: 0.8827 - val_loss: 0.3284 - val_accuracy: 0.8570\n",
      "Epoch 96/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2650 - accuracy: 0.8844 - val_loss: 0.3215 - val_accuracy: 0.8601\n",
      "Epoch 97/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2648 - accuracy: 0.8838 - val_loss: 0.3300 - val_accuracy: 0.8553\n",
      "Epoch 98/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2625 - accuracy: 0.8860 - val_loss: 0.3265 - val_accuracy: 0.8572\n",
      "Epoch 99/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2585 - accuracy: 0.8881 - val_loss: 0.3238 - val_accuracy: 0.8599\n",
      "Epoch 100/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.2579 - accuracy: 0.8884 - val_loss: 0.3186 - val_accuracy: 0.8618\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 256)               3328      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36353 (142.00 KB)\n",
      "Trainable params: 36353 (142.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.4755 - accuracy: 0.7790 - val_loss: 0.4613 - val_accuracy: 0.7837\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4595 - accuracy: 0.7874 - val_loss: 0.4741 - val_accuracy: 0.7772\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4561 - accuracy: 0.7895 - val_loss: 0.4601 - val_accuracy: 0.7862\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4514 - accuracy: 0.7922 - val_loss: 0.4563 - val_accuracy: 0.7947\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4491 - accuracy: 0.7929 - val_loss: 0.4525 - val_accuracy: 0.7920\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4446 - accuracy: 0.7936 - val_loss: 0.4445 - val_accuracy: 0.7952\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4384 - accuracy: 0.7969 - val_loss: 0.4361 - val_accuracy: 0.7963\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4339 - accuracy: 0.7996 - val_loss: 0.4355 - val_accuracy: 0.8005\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4311 - accuracy: 0.8004 - val_loss: 0.4385 - val_accuracy: 0.7898\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4287 - accuracy: 0.8016 - val_loss: 0.4256 - val_accuracy: 0.8058\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4246 - accuracy: 0.8024 - val_loss: 0.4291 - val_accuracy: 0.8010\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4210 - accuracy: 0.8044 - val_loss: 0.4176 - val_accuracy: 0.8066\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4184 - accuracy: 0.8059 - val_loss: 0.4207 - val_accuracy: 0.8062\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4157 - accuracy: 0.8068 - val_loss: 0.4215 - val_accuracy: 0.8046\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4138 - accuracy: 0.8076 - val_loss: 0.4189 - val_accuracy: 0.8053\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4118 - accuracy: 0.8081 - val_loss: 0.4172 - val_accuracy: 0.8064\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4091 - accuracy: 0.8094 - val_loss: 0.4082 - val_accuracy: 0.8109\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4071 - accuracy: 0.8116 - val_loss: 0.4105 - val_accuracy: 0.8108\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4045 - accuracy: 0.8116 - val_loss: 0.4085 - val_accuracy: 0.8119\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4028 - accuracy: 0.8131 - val_loss: 0.4045 - val_accuracy: 0.8136\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.4006 - accuracy: 0.8143 - val_loss: 0.4139 - val_accuracy: 0.8084\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3987 - accuracy: 0.8151 - val_loss: 0.4010 - val_accuracy: 0.8180\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3971 - accuracy: 0.8156 - val_loss: 0.3994 - val_accuracy: 0.8174\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 74289s 31s/step - loss: 0.3952 - accuracy: 0.8164 - val_loss: 0.3966 - val_accuracy: 0.8201\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3934 - accuracy: 0.8185 - val_loss: 0.4004 - val_accuracy: 0.8143\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3922 - accuracy: 0.8186 - val_loss: 0.4049 - val_accuracy: 0.8134\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3907 - accuracy: 0.8189 - val_loss: 0.4018 - val_accuracy: 0.8152\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3889 - accuracy: 0.8198 - val_loss: 0.3942 - val_accuracy: 0.8189\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3874 - accuracy: 0.8204 - val_loss: 0.3923 - val_accuracy: 0.8190\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3864 - accuracy: 0.8216 - val_loss: 0.4133 - val_accuracy: 0.8117\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3846 - accuracy: 0.8227 - val_loss: 0.3887 - val_accuracy: 0.8222\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 6s 3ms/step - loss: 0.3840 - accuracy: 0.8229 - val_loss: 0.3918 - val_accuracy: 0.8202\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.3827 - accuracy: 0.8229 - val_loss: 0.3865 - val_accuracy: 0.8215\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.3814 - accuracy: 0.8236 - val_loss: 0.3960 - val_accuracy: 0.8171\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3808 - accuracy: 0.8243 - val_loss: 0.3873 - val_accuracy: 0.8223\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3789 - accuracy: 0.8250 - val_loss: 0.3886 - val_accuracy: 0.8220\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3788 - accuracy: 0.8244 - val_loss: 0.3947 - val_accuracy: 0.8152\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.3774 - accuracy: 0.8263 - val_loss: 0.3868 - val_accuracy: 0.8201\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3767 - accuracy: 0.8262 - val_loss: 0.3840 - val_accuracy: 0.8243\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3754 - accuracy: 0.8268 - val_loss: 0.3843 - val_accuracy: 0.8258\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3755 - accuracy: 0.8261 - val_loss: 0.3832 - val_accuracy: 0.8232\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3737 - accuracy: 0.8279 - val_loss: 0.3797 - val_accuracy: 0.8274\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3735 - accuracy: 0.8263 - val_loss: 0.3834 - val_accuracy: 0.8244\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.3728 - accuracy: 0.8277 - val_loss: 0.3891 - val_accuracy: 0.8201\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.3717 - accuracy: 0.8283 - val_loss: 0.3779 - val_accuracy: 0.8254\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 12s 5ms/step - loss: 0.3704 - accuracy: 0.8281 - val_loss: 0.3827 - val_accuracy: 0.8244\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 13s 5ms/step - loss: 0.3697 - accuracy: 0.8296 - val_loss: 0.3756 - val_accuracy: 0.8274\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.3695 - accuracy: 0.8297 - val_loss: 0.3756 - val_accuracy: 0.8279\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3688 - accuracy: 0.8296 - val_loss: 0.3761 - val_accuracy: 0.8297\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3683 - accuracy: 0.8285 - val_loss: 0.3836 - val_accuracy: 0.8253\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 8s 4ms/step - loss: 0.3681 - accuracy: 0.8305 - val_loss: 0.3815 - val_accuracy: 0.8261\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3657 - accuracy: 0.8303 - val_loss: 0.3836 - val_accuracy: 0.8266\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 8s 3ms/step - loss: 0.3661 - accuracy: 0.8312 - val_loss: 0.3769 - val_accuracy: 0.8260\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3660 - accuracy: 0.8300 - val_loss: 0.4009 - val_accuracy: 0.8131\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 7s 3ms/step - loss: 0.3647 - accuracy: 0.8309 - val_loss: 0.3790 - val_accuracy: 0.8281\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3644 - accuracy: 0.8314 - val_loss: 0.3772 - val_accuracy: 0.8239\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 9s 4ms/step - loss: 0.3635 - accuracy: 0.8318 - val_loss: 0.3806 - val_accuracy: 0.8257\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               6656      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 179201 (700.00 KB)\n",
      "Trainable params: 179201 (700.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 17s 6ms/step - loss: 0.4785 - accuracy: 0.7782 - val_loss: 0.4612 - val_accuracy: 0.7861\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4584 - accuracy: 0.7846 - val_loss: 0.4634 - val_accuracy: 0.7897\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4516 - accuracy: 0.7895 - val_loss: 0.4499 - val_accuracy: 0.7928\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 17s 7ms/step - loss: 0.4469 - accuracy: 0.7921 - val_loss: 0.4643 - val_accuracy: 0.7785\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.4431 - accuracy: 0.7938 - val_loss: 0.4416 - val_accuracy: 0.7958\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4366 - accuracy: 0.7983 - val_loss: 0.4276 - val_accuracy: 0.8054\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4289 - accuracy: 0.8028 - val_loss: 0.4417 - val_accuracy: 0.7938\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4229 - accuracy: 0.8044 - val_loss: 0.4299 - val_accuracy: 0.7989\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4156 - accuracy: 0.8094 - val_loss: 0.4140 - val_accuracy: 0.8113\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4087 - accuracy: 0.8126 - val_loss: 0.4121 - val_accuracy: 0.8119\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.4025 - accuracy: 0.8130 - val_loss: 0.4057 - val_accuracy: 0.8132\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3958 - accuracy: 0.8160 - val_loss: 0.3968 - val_accuracy: 0.8162\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 14s 6ms/step - loss: 0.3885 - accuracy: 0.8206 - val_loss: 0.3909 - val_accuracy: 0.8234\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3845 - accuracy: 0.8240 - val_loss: 0.3846 - val_accuracy: 0.8222\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3780 - accuracy: 0.8268 - val_loss: 0.4016 - val_accuracy: 0.8143\n",
      "Epoch 16/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3757 - accuracy: 0.8277 - val_loss: 0.4088 - val_accuracy: 0.8149\n",
      "Epoch 17/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3687 - accuracy: 0.8311 - val_loss: 0.3814 - val_accuracy: 0.8244\n",
      "Epoch 18/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3639 - accuracy: 0.8326 - val_loss: 0.3744 - val_accuracy: 0.8291\n",
      "Epoch 19/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3583 - accuracy: 0.8377 - val_loss: 0.3738 - val_accuracy: 0.8299\n",
      "Epoch 20/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3537 - accuracy: 0.8383 - val_loss: 0.3717 - val_accuracy: 0.8295\n",
      "Epoch 21/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3500 - accuracy: 0.8416 - val_loss: 0.3632 - val_accuracy: 0.8352\n",
      "Epoch 22/100\n",
      "2380/2380 [==============================] - 16s 7ms/step - loss: 0.3440 - accuracy: 0.8442 - val_loss: 0.3665 - val_accuracy: 0.8343\n",
      "Epoch 23/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3404 - accuracy: 0.8463 - val_loss: 0.3758 - val_accuracy: 0.8282\n",
      "Epoch 24/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3350 - accuracy: 0.8491 - val_loss: 0.3542 - val_accuracy: 0.8363\n",
      "Epoch 25/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3329 - accuracy: 0.8495 - val_loss: 0.3644 - val_accuracy: 0.8382\n",
      "Epoch 26/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3317 - accuracy: 0.8494 - val_loss: 0.3554 - val_accuracy: 0.8415\n",
      "Epoch 27/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3238 - accuracy: 0.8558 - val_loss: 0.3484 - val_accuracy: 0.8426\n",
      "Epoch 28/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3216 - accuracy: 0.8568 - val_loss: 0.3434 - val_accuracy: 0.8448\n",
      "Epoch 29/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3163 - accuracy: 0.8588 - val_loss: 0.3454 - val_accuracy: 0.8452\n",
      "Epoch 30/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3144 - accuracy: 0.8593 - val_loss: 0.3365 - val_accuracy: 0.8519\n",
      "Epoch 31/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3088 - accuracy: 0.8628 - val_loss: 0.3370 - val_accuracy: 0.8493\n",
      "Epoch 32/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3060 - accuracy: 0.8644 - val_loss: 0.3300 - val_accuracy: 0.8560\n",
      "Epoch 33/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.3024 - accuracy: 0.8657 - val_loss: 0.3368 - val_accuracy: 0.8517\n",
      "Epoch 34/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2987 - accuracy: 0.8679 - val_loss: 0.3269 - val_accuracy: 0.8563\n",
      "Epoch 35/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2958 - accuracy: 0.8683 - val_loss: 0.3491 - val_accuracy: 0.8434\n",
      "Epoch 36/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2884 - accuracy: 0.8740 - val_loss: 0.3585 - val_accuracy: 0.8404\n",
      "Epoch 37/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2866 - accuracy: 0.8739 - val_loss: 0.3393 - val_accuracy: 0.8480\n",
      "Epoch 38/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2831 - accuracy: 0.8762 - val_loss: 0.3336 - val_accuracy: 0.8511\n",
      "Epoch 39/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2809 - accuracy: 0.8776 - val_loss: 0.3375 - val_accuracy: 0.8486\n",
      "Epoch 40/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2751 - accuracy: 0.8804 - val_loss: 0.3242 - val_accuracy: 0.8583\n",
      "Epoch 41/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2714 - accuracy: 0.8807 - val_loss: 0.3099 - val_accuracy: 0.8664\n",
      "Epoch 42/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2684 - accuracy: 0.8845 - val_loss: 0.3367 - val_accuracy: 0.8536\n",
      "Epoch 43/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2681 - accuracy: 0.8835 - val_loss: 0.3146 - val_accuracy: 0.8624\n",
      "Epoch 44/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2613 - accuracy: 0.8874 - val_loss: 0.3136 - val_accuracy: 0.8679\n",
      "Epoch 45/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2604 - accuracy: 0.8874 - val_loss: 0.3272 - val_accuracy: 0.8577\n",
      "Epoch 46/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2581 - accuracy: 0.8891 - val_loss: 0.3473 - val_accuracy: 0.8473\n",
      "Epoch 47/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2519 - accuracy: 0.8917 - val_loss: 0.3031 - val_accuracy: 0.8711\n",
      "Epoch 48/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2448 - accuracy: 0.8952 - val_loss: 0.3169 - val_accuracy: 0.8656\n",
      "Epoch 49/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2455 - accuracy: 0.8958 - val_loss: 0.3547 - val_accuracy: 0.8465\n",
      "Epoch 50/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2421 - accuracy: 0.8965 - val_loss: 0.3012 - val_accuracy: 0.8720\n",
      "Epoch 51/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2413 - accuracy: 0.8983 - val_loss: 0.3325 - val_accuracy: 0.8627\n",
      "Epoch 52/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2320 - accuracy: 0.9018 - val_loss: 0.3139 - val_accuracy: 0.8684\n",
      "Epoch 53/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2346 - accuracy: 0.9007 - val_loss: 0.3064 - val_accuracy: 0.8720\n",
      "Epoch 54/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2290 - accuracy: 0.9034 - val_loss: 0.3255 - val_accuracy: 0.8628\n",
      "Epoch 55/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2233 - accuracy: 0.9075 - val_loss: 0.3356 - val_accuracy: 0.8575\n",
      "Epoch 56/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2268 - accuracy: 0.9030 - val_loss: 0.3154 - val_accuracy: 0.8702\n",
      "Epoch 57/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2201 - accuracy: 0.9086 - val_loss: 0.3174 - val_accuracy: 0.8714\n",
      "Epoch 58/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2172 - accuracy: 0.9088 - val_loss: 0.2909 - val_accuracy: 0.8812\n",
      "Epoch 59/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2106 - accuracy: 0.9132 - val_loss: 0.3029 - val_accuracy: 0.8768\n",
      "Epoch 60/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2124 - accuracy: 0.9109 - val_loss: 0.2958 - val_accuracy: 0.8800\n",
      "Epoch 61/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2059 - accuracy: 0.9145 - val_loss: 0.2967 - val_accuracy: 0.8798\n",
      "Epoch 62/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2032 - accuracy: 0.9159 - val_loss: 0.3056 - val_accuracy: 0.8764\n",
      "Epoch 63/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.2028 - accuracy: 0.9147 - val_loss: 0.2891 - val_accuracy: 0.8886\n",
      "Epoch 64/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1951 - accuracy: 0.9202 - val_loss: 0.3062 - val_accuracy: 0.8762\n",
      "Epoch 65/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1950 - accuracy: 0.9205 - val_loss: 0.2877 - val_accuracy: 0.8858\n",
      "Epoch 66/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1918 - accuracy: 0.9215 - val_loss: 0.2918 - val_accuracy: 0.8839\n",
      "Epoch 67/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1884 - accuracy: 0.9227 - val_loss: 0.3013 - val_accuracy: 0.8800\n",
      "Epoch 68/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1868 - accuracy: 0.9231 - val_loss: 0.2950 - val_accuracy: 0.8850\n",
      "Epoch 69/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1845 - accuracy: 0.9250 - val_loss: 0.2932 - val_accuracy: 0.8888\n",
      "Epoch 70/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1798 - accuracy: 0.9274 - val_loss: 0.2817 - val_accuracy: 0.8921\n",
      "Epoch 71/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1805 - accuracy: 0.9268 - val_loss: 0.3098 - val_accuracy: 0.8784\n",
      "Epoch 72/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1782 - accuracy: 0.9289 - val_loss: 0.3175 - val_accuracy: 0.8795\n",
      "Epoch 73/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1719 - accuracy: 0.9315 - val_loss: 0.3135 - val_accuracy: 0.8781\n",
      "Epoch 74/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1705 - accuracy: 0.9324 - val_loss: 0.3229 - val_accuracy: 0.8745\n",
      "Epoch 75/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1726 - accuracy: 0.9305 - val_loss: 0.2799 - val_accuracy: 0.8953\n",
      "Epoch 76/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1627 - accuracy: 0.9350 - val_loss: 0.2882 - val_accuracy: 0.8916\n",
      "Epoch 77/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1624 - accuracy: 0.9351 - val_loss: 0.2993 - val_accuracy: 0.8829\n",
      "Epoch 78/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1630 - accuracy: 0.9349 - val_loss: 0.2995 - val_accuracy: 0.8898\n",
      "Epoch 79/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1579 - accuracy: 0.9371 - val_loss: 0.2899 - val_accuracy: 0.8907\n",
      "Epoch 80/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1565 - accuracy: 0.9374 - val_loss: 0.2778 - val_accuracy: 0.8985\n",
      "Epoch 81/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1544 - accuracy: 0.9384 - val_loss: 0.2942 - val_accuracy: 0.8951\n",
      "Epoch 82/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1547 - accuracy: 0.9389 - val_loss: 0.2953 - val_accuracy: 0.8892\n",
      "Epoch 83/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1496 - accuracy: 0.9407 - val_loss: 0.2988 - val_accuracy: 0.8899\n",
      "Epoch 84/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1458 - accuracy: 0.9429 - val_loss: 0.2962 - val_accuracy: 0.8931\n",
      "Epoch 85/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1518 - accuracy: 0.9399 - val_loss: 0.3072 - val_accuracy: 0.8905\n",
      "Epoch 86/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1426 - accuracy: 0.9431 - val_loss: 0.2764 - val_accuracy: 0.9020\n",
      "Epoch 87/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1439 - accuracy: 0.9428 - val_loss: 0.2880 - val_accuracy: 0.8987\n",
      "Epoch 88/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1398 - accuracy: 0.9461 - val_loss: 0.3181 - val_accuracy: 0.8936\n",
      "Epoch 89/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1368 - accuracy: 0.9461 - val_loss: 0.2863 - val_accuracy: 0.8993\n",
      "Epoch 90/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1349 - accuracy: 0.9470 - val_loss: 0.2880 - val_accuracy: 0.9046\n",
      "Epoch 91/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1351 - accuracy: 0.9463 - val_loss: 0.2895 - val_accuracy: 0.8991\n",
      "Epoch 92/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1333 - accuracy: 0.9480 - val_loss: 0.3495 - val_accuracy: 0.8724\n",
      "Epoch 93/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1247 - accuracy: 0.9509 - val_loss: 0.3079 - val_accuracy: 0.8975\n",
      "Epoch 94/100\n",
      "2380/2380 [==============================] - 15s 6ms/step - loss: 0.1289 - accuracy: 0.9504 - val_loss: 0.3441 - val_accuracy: 0.8809\n",
      "Epoch 95/100\n",
      " 325/2380 [===>..........................] - ETA: 11s - loss: 0.1218 - accuracy: 0.9533"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chand\\OneDrive\\Desktop\\Gait Project\\Code\\modeltesting.ipynb Cell 31\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     X_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X42sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "model.add(layers.Dense(512, activation='tanh'))\n",
    "model.add(layers.Dense(256, activation='tanh'))\n",
    "model.add(layers.Dense(128, activation='tanh'))\n",
    "model.add(layers.Dense(64, activation='tanh'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 256)               3328      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44545 (174.00 KB)\n",
      "Trainable params: 44545 (174.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2380/2380 [==============================] - 12s 4ms/step - loss: 0.6310 - accuracy: 0.6066 - val_loss: 0.4871 - val_accuracy: 0.7730\n",
      "Epoch 2/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4845 - accuracy: 0.7754 - val_loss: 0.4774 - val_accuracy: 0.7786\n",
      "Epoch 3/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4788 - accuracy: 0.7778 - val_loss: 0.4754 - val_accuracy: 0.7788\n",
      "Epoch 4/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4770 - accuracy: 0.7803 - val_loss: 0.4794 - val_accuracy: 0.7765\n",
      "Epoch 5/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4743 - accuracy: 0.7819 - val_loss: 0.4752 - val_accuracy: 0.7807\n",
      "Epoch 6/100\n",
      "2380/2380 [==============================] - 10s 4ms/step - loss: 0.4748 - accuracy: 0.7814 - val_loss: 0.4768 - val_accuracy: 0.7807\n",
      "Epoch 7/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4741 - accuracy: 0.7828 - val_loss: 0.4742 - val_accuracy: 0.7798\n",
      "Epoch 8/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4730 - accuracy: 0.7827 - val_loss: 0.4962 - val_accuracy: 0.7728\n",
      "Epoch 9/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4723 - accuracy: 0.7830 - val_loss: 0.4823 - val_accuracy: 0.7792\n",
      "Epoch 10/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4719 - accuracy: 0.7835 - val_loss: 0.4824 - val_accuracy: 0.7745\n",
      "Epoch 11/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4721 - accuracy: 0.7831 - val_loss: 0.4745 - val_accuracy: 0.7810\n",
      "Epoch 12/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4718 - accuracy: 0.7827 - val_loss: 0.4772 - val_accuracy: 0.7790\n",
      "Epoch 13/100\n",
      "2380/2380 [==============================] - 11s 4ms/step - loss: 0.4720 - accuracy: 0.7835 - val_loss: 0.4742 - val_accuracy: 0.7814\n",
      "Epoch 14/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4719 - accuracy: 0.7832 - val_loss: 0.4778 - val_accuracy: 0.7781\n",
      "Epoch 15/100\n",
      "2380/2380 [==============================] - 11s 5ms/step - loss: 0.4708 - accuracy: 0.7835 - val_loss: 0.4733 - val_accuracy: 0.7810\n",
      "Epoch 16/100\n",
      " 574/2380 [======>.......................] - ETA: 7s - loss: 0.4704 - accuracy: 0.7833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chand\\OneDrive\\Desktop\\Gait Project\\Code\\modeltesting.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     X_train, y_train,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stopping],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chand/OneDrive/Desktop/Gait%20Project/Code/modeltesting.ipynb#X43sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=(12,)))\n",
    "\n",
    "model.add(layers.Dense(256, activation='sigmoid'))\n",
    "model.add(layers.Dense(128, activation='sigmoid'))\n",
    "model.add(layers.Dense(64, activation='sigmoid'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adadelta optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
